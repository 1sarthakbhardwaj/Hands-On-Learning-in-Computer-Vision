{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "613f4c81",
   "metadata": {},
   "source": [
    "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
    "\n",
    "# Segment Anything Model 2 (SAM2)\n",
    "\n",
    "---\n",
    "\n",
    "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog/<BLOG_NAME>)\n",
    "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
    "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)\n",
    "[![Scientific Paper](https://img.shields.io/badge/Official-Paper-blue.svg)](<PAPER LINK>)\n",
    "\n",
    "# About SAM2\n",
    "Segment Anything Model 2 (SAM2) is an advanced AI model designed to overcome limitations of its predecessor, SAM, by excelling in both image and video segmentation. Built on a transformer architecture with a streaming memory system, SAM2 processes video in real-time, tracking objects across frames using optical flow and temporal attention. It achieves up to 6x higher accuracy in image tasks through multi-scale feature fusion and high-resolution mask decoders, capturing fine details and small objects. Optimized for efficiency via lightweight variants and sparse attention, SAM2 reduces computational overhead. Trained on the SA-V dataset (10M+ videos), it leverages a model-in-the-loop data engine for iterative improvement and supports multi-modal prompts (clicks, text). Applications span real-time video analytics, medical imaging, and robotics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e94bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! uv pip install -q torch torchvision numpy matplotlib opencv-python pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3218ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import requests\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62fa53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install 'git+https://github.com/facebookresearch/sam2.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba8dbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../checkpoints/\n",
    "!wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf74a28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f260dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "sam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=device)\n",
    "\n",
    "predictor = SAM2ImagePredictor(sam2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a040407",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# !wget -O elephant.jpg http://farm8.staticflickr.com/7193/6956100130_8bfc1afaa1_z.jpg\n",
    "# masks, scores = box_segmentor(\n",
    "#     image_path=\"elephant.jpg\",\n",
    "#     box_coords=[ 95.1795, 156.5467, 255.3454, 307.8629]\n",
    "# )\n",
    "\n",
    "# masks, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd76176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def box_segmentor(image_path, box_coords, model_checkpoint=None, device='auto'):\n",
    "    \"\"\"\n",
    "    Perform SAM2 segmentation using multiple box coordinates with optimized processing.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to input image\n",
    "        box_coords (list): List of [x_min, y_min, x_max, y_max] coordinates\n",
    "        model_checkpoint (str): Optional path to SAM2 checkpoint\n",
    "        device (str): 'cuda', 'cpu' or 'auto' (default)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (masks_list, scores_list) containing segmentation results\n",
    "    \"\"\"\n",
    "    # Device configuration\n",
    "    if device == 'auto':\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load model (if not already loaded)\n",
    "    if not hasattr(box_segmentor, 'predictor'):\n",
    "        checkpoint = model_checkpoint or \"../checkpoints/sam2.1_hiera_large.pt\"\n",
    "        model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "        \n",
    "        sam2_model = build_sam2(model_cfg, checkpoint, device=device)\n",
    "        box_segmentor.predictor = SAM2ImagePredictor(sam2_model)\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    image = np.array(Image.open(requests.get(image_path, stream=True).raw))\n",
    "    \n",
    "    # Set image once for all predictions (optimization)\n",
    "    with torch.inference_mode(), torch.autocast(device.type, dtype=torch.bfloat16):\n",
    "        box_segmentor.predictor.set_image(image)\n",
    "        \n",
    "        masks_list = []\n",
    "        scores_list = []\n",
    "        \n",
    "        for box in box_coords:\n",
    "            box_array = np.array(box)[None, :]  # Add batch dimension\n",
    "            \n",
    "            # Correct unpacking: predict returns (masks, scores, logits)\n",
    "            predicted_masks, predicted_scores, _ = box_segmentor.predictor.predict(\n",
    "                box=box_array,\n",
    "                multimask_output=False\n",
    "            )\n",
    "            \n",
    "            masks_list.append(predicted_masks[0])\n",
    "            scores_list.append(predicted_scores[0])\n",
    "    \n",
    "    return masks_list, scores_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316209d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_masks(image, masks, alpha=0.5, random_color=True, borders=True):\n",
    "    \"\"\"\n",
    "    Display multiple masks over an image with different colors\n",
    "    \n",
    "    Args:\n",
    "        image: PIL.Image - Original image\n",
    "        masks: List of 2D numpy arrays (float32 masks 0-1)\n",
    "        alpha: float (0-1) - Transparency of mask fills\n",
    "        random_color: bool - Use random colors for each mask\n",
    "        borders: bool - Show border contours\n",
    "    \"\"\"\n",
    "    # Convert image to numpy array\n",
    "    img_np = np.array(image.convert(\"RGB\"))\n",
    "    h, w = img_np.shape[:2]\n",
    "    \n",
    "    # Create overlay canvas\n",
    "    overlay = np.zeros((h, w, 4), dtype=np.float32)\n",
    "    \n",
    "    # Generate colors for each mask\n",
    "    colors = []\n",
    "    for _ in masks:\n",
    "        if random_color:\n",
    "            colors.append(np.append(np.random.random(3), alpha))\n",
    "        else:\n",
    "            colors.append(np.array([30/255, 144/255, 255/255, alpha]))\n",
    "    \n",
    "    # Process each mask\n",
    "    for i, mask in enumerate(masks):\n",
    "        # Convert mask to binary\n",
    "        binary_mask = (mask > 0.5).astype(np.uint8)\n",
    "        \n",
    "        # Create mask overlay\n",
    "        mask_overlay = np.zeros((h, w, 4))\n",
    "        mask_overlay[binary_mask == 1] = colors[i]\n",
    "        \n",
    "        # Add borders\n",
    "        if borders:\n",
    "            contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            contours = [cv2.approxPolyDP(c, 0.01*cv2.arcLength(c, True), True) for c in contours]\n",
    "            \n",
    "            # Draw white borders (80% opacity)\n",
    "            border_color = (1, 1, 1, 0.8)  # White borders\n",
    "            cv2.drawContours(mask_overlay, contours, -1, border_color, 2)\n",
    "        \n",
    "        # Combine overlays\n",
    "        overlay = np.where(mask_overlay != 0, mask_overlay, overlay)\n",
    "    \n",
    "    # Create final composition\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(img_np)\n",
    "    ax.imshow(overlay)\n",
    "    ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb91026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import random\n",
    "\n",
    "def draw_boxes_on_image(image, input_boxes, colors=None, line_width=3):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes on an image with random colors\n",
    "    \n",
    "    Args:\n",
    "        image: PIL.Image or file path\n",
    "        input_boxes: List of boxes in [x_min, y_min, x_max, y_max] format\n",
    "        colors: None (random), single color, or list of colors (optional)\n",
    "        line_width: Width of box borders in pixels (default: 3)\n",
    "        display: Whether to display the image (default: False)\n",
    "    \n",
    "    Returns:\n",
    "        PIL.Image with drawn boxes\n",
    "    \"\"\"\n",
    "    # Convert to PIL Image if path provided\n",
    "    if isinstance(image, str):\n",
    "        image = Image.open(image).convert(\"RGB\")\n",
    "    \n",
    "    # Validate input boxes\n",
    "    if not all(len(box) == 4 for box in input_boxes):\n",
    "        raise ValueError(\"Boxes must be in [x_min, y_min, x_max, y_max] format\")\n",
    "    \n",
    "    # Convert coordinates to integers\n",
    "    boxes = [tuple(map(int, box)) for box in input_boxes]\n",
    "    \n",
    "    # Generate random colors if none provided\n",
    "    if colors is None:\n",
    "        colors = [\n",
    "            (\n",
    "                random.randint(0, 255),\n",
    "                random.randint(0, 255),\n",
    "                random.randint(0, 255)\n",
    "            ) for _ in input_boxes\n",
    "        ]\n",
    "    # Handle single color input\n",
    "    elif isinstance(colors, (str, tuple)):\n",
    "        colors = [colors] * len(input_boxes)\n",
    "    \n",
    "    # Verify color/box count match\n",
    "    if len(colors) != len(input_boxes):\n",
    "        raise ValueError(\"Number of colors must match number of boxes\")\n",
    "    \n",
    "    # Draw boxes\n",
    "    image = image.copy()\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for box, color in zip(boxes, colors):\n",
    "        draw.rectangle(box, outline=color, width=line_width)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee55c6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = \"https://farm8.staticflickr.com/7331/9239614558_26e5a50351_z.jpg\"\n",
    "img1 = Image.open(requests.get(url1, stream=True).raw)\n",
    "img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb8fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_boxes = [[308.9709, 116.7895, 503.7764, 258.8303],\n",
    "          [172.0639, 186.9951, 297.3459, 328.9594]]\n",
    "masks, scores = box_segmentor(\n",
    "    image_path=url1,\n",
    "    box_coords= input_boxes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9a5fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_boxes_on_image(\n",
    "    image=img1,\n",
    "    input_boxes=input_boxes,\n",
    "    line_width=2,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6769830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_masks(img1, masks, alpha=0.5, random_color=True, borders=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7790e642",
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = \"http://farm6.staticflickr.com/5455/9293164411_47fae6c6cb_z.jpg\"\n",
    "img2 = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "input_boxes = [[178.88, 97.84, 586.23, 395.83]]\n",
    "\n",
    "masks, scores = box_segmentor(image_path = url2, box_coords = input_boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a853a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_boxes_on_image(image=img2, input_boxes=input_boxes, line_width=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4046d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_masks(img2, masks, alpha=0.5, random_color=True, borders=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1403b549",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dd1945",
   "metadata": {},
   "source": [
    "## using point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46d3b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_mask(mask, ax, random_color=False, borders = True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask = mask.astype(np.uint8)\n",
    "    mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    if borders:\n",
    "        import cv2\n",
    "        contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "        # Try to smooth contours\n",
    "        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2) \n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))\n",
    "\n",
    "def show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):\n",
    "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)\n",
    "        show_mask(mask, plt.gca(), borders=borders)\n",
    "        if point_coords is not None:\n",
    "            assert input_labels is not None\n",
    "            show_points(point_coords, input_labels, plt.gca())\n",
    "        if box_coords is not None:\n",
    "            # boxes\n",
    "            show_box(box_coords, plt.gca())\n",
    "        if len(scores) > 1:\n",
    "            plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ff182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_segmentor(\n",
    "    image,\n",
    "    point_sets,\n",
    "    label_sets,\n",
    "    predictor,\n",
    "    show_individual=False,\n",
    "    show_combined=True,\n",
    "    multimask_output=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Segments objects in an image using multiple sets of points with SAM2 and visualizes the results.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The input image as a numpy array (H, W, 3).\n",
    "        point_sets (list of np.ndarray): List of (N_points, 2) arrays, each array is a set of (x, y) points.\n",
    "        label_sets (list of np.ndarray): List of (N_points,) arrays, each array is a set of labels (1=fg, 0=bg) for corresponding points.\n",
    "        predictor (SAM2ImagePredictor): Predictor object.\n",
    "        show_individual (bool): If True, show each segmentation result individually.\n",
    "        show_combined (bool): If True, show all masks overlaid on the image.\n",
    "        multimask_output (bool): If True, returns multiple masks per prompt set.\n",
    "\n",
    "    Returns:\n",
    "        all_masks (list): List of mask arrays for each point set.\n",
    "        all_scores (list): List of scores for each mask.\n",
    "    \"\"\"\n",
    "    predictor.set_image(image)\n",
    "    all_masks = []\n",
    "    all_scores = []\n",
    "\n",
    "    # Predict masks for each set of points\n",
    "    for pts, lbls in zip(point_sets, label_sets):\n",
    "        masks, scores, _ = predictor.predict(\n",
    "            point_coords=pts,\n",
    "            point_labels=lbls,\n",
    "            multimask_output=multimask_output\n",
    "        )\n",
    "        all_masks.append(masks)\n",
    "        all_scores.append(scores)\n",
    "\n",
    "    # Visualize each mask individually\n",
    "    if show_individual:\n",
    "        for i, (masks, scores, pts, lbls) in enumerate(zip(all_masks, all_scores, point_sets, label_sets)):\n",
    "            show_masks(\n",
    "                image,\n",
    "                masks,\n",
    "                scores,\n",
    "                point_coords=pts,\n",
    "                input_labels=lbls,\n",
    "                borders=True\n",
    "            )\n",
    "\n",
    "    # Visualize all best masks together\n",
    "    if show_combined:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)\n",
    "        for masks, pts, lbls in zip(all_masks, point_sets, label_sets):\n",
    "            # Use the best mask (highest score)\n",
    "            best_idx = np.argmax(all_scores[all_masks.index(masks)])\n",
    "            show_mask(masks[best_idx], plt.gca(), random_color=True)\n",
    "            show_points(pts, lbls, plt.gca())\n",
    "        plt.title(\"All Segmented Objects (Best Masks)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return all_masks, all_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91d7855",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e7fe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: segmenting two objects with different point prompts\n",
    "point_sets = [\n",
    "    np.array([[406, 187 ], [220, 258]]) # Points for object\n",
    "]\n",
    "label_sets = [\n",
    "    np.array([1, 1])                   # Labels for object\n",
    "]\n",
    "\n",
    "masks, scores = point_segmentor(img1, point_sets, label_sets, predictor, show_combined = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5d4bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d42102",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_sets = [\n",
    "    np.array([[350, 246]]) # Points for object\n",
    "]\n",
    "label_sets = [\n",
    "    np.array([1])                   # Labels for object\n",
    "]\n",
    "\n",
    "masks, scores = point_segmentor(img2, point_sets, label_sets, predictor, show_combined = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
