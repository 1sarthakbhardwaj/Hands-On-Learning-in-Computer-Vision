{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2022494f",
   "metadata": {},
   "source": [
    "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
    "\n",
    "# **Twitter Post Generator Agent**\n",
    "\n",
    "---\n",
    "\n",
    "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog/<BLOG_NAME>)\n",
    "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
    "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f095b99a",
   "metadata": {},
   "source": [
    "This notebook implements an automated system for generating Twitter/X posts from either blog content or YouTube videos using CrewAI and Google's Gemini model. The system extracts content from URLs and generates engaging, concise social media posts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1675043",
   "metadata": {},
   "source": [
    "## Dependencies Installation\n",
    "\n",
    "This cell installs the required Python packages:\n",
    "- crewai: Framework for creating AI agent workflows\n",
    "- python-dotenv: For managing environment variables\n",
    "- requests: For making HTTP requests\n",
    "- beautifulsoup4: For parsing HTML content\n",
    "- youtube-transcript-api: For extracting YouTube video transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "887f9d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Install Dependencies =====\n",
    "# !pip install crewai python-dotenv requests beautifulsoup4 youtube-transcript-api\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bdf1fc",
   "metadata": {},
   "source": [
    "## Import Required Libraries and Modules\n",
    "\n",
    "This cell imports all necessary libraries for:\n",
    "- CrewAI components (Agent, Task, Crew, Process, LLM)\n",
    "- Web scraping and HTTP requests (requests, BeautifulSoup)\n",
    "- Environment configuration (os, dotenv)\n",
    "- URL parsing and text manipulation (re, urllib.parse)\n",
    "- YouTube transcript extraction (YouTubeTranscriptApi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "685b64a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Imports and Environment setup =====\n",
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "from crewai.tools import tool\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafcb814",
   "metadata": {},
   "source": [
    "## API Key Configuration\n",
    "\n",
    "This cell loads the Google Gemini API key from a .env file. Make sure to:\n",
    "1. Create a .env file in the same directory as this notebook\n",
    "2. Add your Gemini API key: `GEMINI_API_KEY=your_key_here`\n",
    "3. Keep the .env file secure and never commit it to version control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a149acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Load API Key from .env =====\n",
    "# Make sure you have a .env file with a line: GEMINI_API_KEY=your_key_here\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e632129",
   "metadata": {},
   "source": [
    "## Blog Content Extraction Tool\n",
    "\n",
    "This tool is responsible for extracting textual content from blog posts and web articles:\n",
    "- Makes an HTTP request to the provided URL\n",
    "- Parses HTML using BeautifulSoup4\n",
    "- Extracts text from paragraph (`<p>`) tags\n",
    "- Falls back to body text if insufficient content is found\n",
    "- Limits output to 4000 characters\n",
    "- Handles errors gracefully with informative messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "57aa108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Blog Content Extractor Tool =====\n",
    "@tool('Content Extractor')\n",
    "def extract_content_from_url(url: str) -> str:\n",
    "    '''Extract main textual info from blog URLs.'''\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        text = '\\n'.join([p.get_text(separator=' ', strip=True) for p in soup.find_all('p')])\n",
    "        if len(text) < 200 and soup.body:\n",
    "            text = soup.body.get_text(separator=' ', strip=True)\n",
    "        return text[:4000]\n",
    "    except Exception as e:\n",
    "        return f\"ERROR extracting content: {e}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea1635d",
   "metadata": {},
   "source": [
    "## YouTube Transcript Extraction Tool\n",
    "\n",
    "This tool handles the extraction of transcripts from YouTube videos:\n",
    "- Supports multiple URL formats (watch, shorts, embed, live, mobile)\n",
    "- Extracts the 11-character video ID using regex patterns\n",
    "- Attempts to fetch English transcripts first\n",
    "- Falls back to auto-generated transcripts if needed\n",
    "- Returns concatenated transcript text\n",
    "- Includes error handling for invalid URLs or missing transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "cc085566",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"YouTube Transcript Extractor\")\n",
    "def extract_youtube_transcript(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract transcript from any YouTube URL format.\n",
    "    \n",
    "    Supports: watch, youtu.be, embed, shorts, live, mobile URLs.\n",
    "    Returns up to 4000 characters of English transcript text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract 11-char video ID from any YouTube URL format\n",
    "        video_id = None\n",
    "        \n",
    "        # Try query parameter first (watch URLs)\n",
    "        if 'v=' in url:\n",
    "            video_id = re.search(r'v=([a-zA-Z0-9_-]{11})', url)\n",
    "        \n",
    "        # Try path-based formats (youtu.be, embed, shorts, live, v)\n",
    "        if not video_id:\n",
    "            video_id = re.search(r'(?:youtu\\.be/|embed/|shorts/|live/|/v/)([a-zA-Z0-9_-]{11})', url)\n",
    "        \n",
    "        if not video_id:\n",
    "            return \"ERROR: Could not extract video ID from URL.\"\n",
    "        \n",
    "        # print(video_id.group(1))\n",
    "        \n",
    "        video_id = video_id.group(1)\n",
    "        \n",
    "        api = YouTubeTranscriptApi()\n",
    "        transcript_list = api.list(video_id)\n",
    "\n",
    "        # Try fetching English transcript, fallback to auto-generated\n",
    "        try:\n",
    "            transcript = transcript_list.find_transcript(['en']).fetch()\n",
    "        except Exception:\n",
    "            transcript = transcript_list.find_generated_transcript(['en']).fetch()\n",
    "\n",
    "        # Each entry is now a FetchedTranscriptSnippet object\n",
    "        # Access its .text attribute instead of ['text']\n",
    "        text = ' '.join([snippet.text for snippet in transcript])\n",
    "\n",
    "        return text\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2745e986",
   "metadata": {},
   "source": [
    "## LLM Configuration\n",
    "\n",
    "Setting up two instances of Google's Gemini model with different configurations:\n",
    "1. **Extracting LLM** (gemini-2.0-flash-lite):\n",
    "   - Used for content extraction and summarization\n",
    "   - Low temperature (0.1) for more focused, deterministic output\n",
    "\n",
    "2. **Writing LLM** (gemini-2.5-flash):\n",
    "   - Used for creative Twitter post generation\n",
    "   - Higher temperature (0.3) for more creative output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "60b79bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LLM Setup =====\n",
    "extracting_llm = LLM(model='gemini/gemini-2.0-flash-exp', \n",
    "                     apikey=GEMINI_API_KEY, \n",
    "                     temperature=0.1)\n",
    "writing_llm = LLM(model='gemini/gemini-2.5-flash', \n",
    "                  apikey=GEMINI_API_KEY, \n",
    "                  temperature=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5feab2",
   "metadata": {},
   "source": [
    "## Agent Definitions\n",
    "\n",
    "Creating two specialized AI agents:\n",
    "\n",
    "1. **Content Extractor Agent**:\n",
    "   - Role: Extracts and processes content from URLs\n",
    "   - Tools: Blog content and YouTube transcript extractors\n",
    "   - Uses the more precise gemini-2.0-flash-lite model\n",
    "\n",
    "2. **Twitter Writer Agent**:\n",
    "   - Role: Crafts engaging Twitter/X posts\n",
    "   - Specializes in viral content with hooks and hashtags\n",
    "   - Uses the more creative gemini-2.5-flash model\n",
    "   - Ensures posts stay within 280-character limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ed6dff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Agent Definitions =====\n",
    "extractor_agent = Agent(\n",
    "    role='Content Extractor',\n",
    "    goal='Extract main points from either blog or YouTube video URLs using the appropriate extraction method.',\n",
    "    backstory='Handles both web articles/blogs and YouTube videos, providing a clean summary.',\n",
    "    tools=[extract_content_from_url, extract_youtube_transcript],\n",
    "    llm=extracting_llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "twitter_writer_agent = Agent(\n",
    "    role='Twitter/X Post Writer',\n",
    "    goal='Draft a concise, viral Twitter/X post from the extracted content (max 280 chars, emojis, hashtags, and link).',\n",
    "    backstory='Expert in generating Twitter posts with actionable hook and hashtags.',\n",
    "    llm=writing_llm,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ef9d68",
   "metadata": {},
   "source": [
    "## Task Definitions\n",
    "\n",
    "Configuring two sequential tasks for the workflow:\n",
    "\n",
    "1. **Content Extraction Task**:\n",
    "   - Extracts and summarizes content from the provided URL\n",
    "   - Handles both blog posts and YouTube videos\n",
    "   - Creates a concise summary of main points\n",
    "\n",
    "2. **Twitter Post Writing Task**:\n",
    "   - Uses the extracted content as context\n",
    "   - Creates a Twitter/X post with:\n",
    "     - Key points from the content\n",
    "     - Appropriate emojis\n",
    "     - Relevant hashtags\n",
    "     - Original URL\n",
    "   - Ensures the post stays within Twitter's 280-character limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ae8fd887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Tasks =====\n",
    "extract_task = Task(\n",
    "    description='Extract and summarize the core topic and points from this URL: {url}',\n",
    "    expected_output='Summary of main points from the content.',\n",
    "    agent=extractor_agent\n",
    ")\n",
    "\n",
    "write_task = Task(\n",
    "    description='Write a Twitter/X post (max 280 chars) with key points, emojis, hashtags, and include this URL: {url}',\n",
    "    expected_output='Ready-to-use twitter post copy, where each are in separate line.',\n",
    "    agent=twitter_writer_agent,\n",
    "    context=[extract_task]  # This ensures it gets the extraction results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085f39a9",
   "metadata": {},
   "source": [
    "## Crew Assembly\n",
    "\n",
    "Creating the CrewAI workflow:\n",
    "- Combines both agents (Content Extractor and Twitter Writer)\n",
    "- Configures sequential task execution\n",
    "- Enables verbose mode for detailed execution tracking\n",
    "- Ensures proper data flow between tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ede29f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Assemble Crew Workflow =====\n",
    "twitter_crew = Crew(\n",
    "    agents=[extractor_agent, twitter_writer_agent],\n",
    "    tasks=[extract_task, write_task],\n",
    "    process=Process.sequential,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a12718",
   "metadata": {},
   "source": [
    "## Execution\n",
    "\n",
    "Run the Twitter post generation workflow:\n",
    "1. Provide a URL (blog post or YouTube video)\n",
    "2. Content is extracted and summarized\n",
    "3. A Twitter/X post is generated\n",
    "4. Final post is printed with the result\n",
    "\n",
    "Example uses a YouTube video URL, but you can replace it with any blog post URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d23be569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Struggling with multi-agent AI? ðŸ¤¯ AIOS (AI Agent Operating System) is your solution! It brings OS-level management to LLM-powered agents for scalable, collaborative AI.\n",
      "ðŸ”— https://www.labellerr.com/blog/aios-explained/\n",
      "#AIOS #AIAutomation #MultiAgentAI #LLM #Tech\n"
     ]
    }
   ],
   "source": [
    "# Run the workflow with a blog or YouTube URL\n",
    "# url = \"https://www.youtube.com/watch?v=GWB9ApTPTv4\"\n",
    "url = \"https://www.labellerr.com/blog/aios-explained/\"\n",
    "result = twitter_crew.kickoff(inputs={'url': url})\n",
    "\n",
    "print(result.raw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
