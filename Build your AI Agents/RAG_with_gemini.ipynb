{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60c091a",
   "metadata": {},
   "source": [
    "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
    "\n",
    "# **Agent 101 ‚Äì Mini RAG + Gemini LLM (In-Memory Vector Store)**\n",
    "\n",
    "---\n",
    "\n",
    "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog/<BLOG_NAME>)\n",
    "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
    "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bf4d99",
   "metadata": {
    "id": "c5bf4d99"
   },
   "source": [
    "## Understanding Retrieval-Augmented Generation (RAG) with LLMs\n",
    "\n",
    "This notebook demonstrates how **Retrieval-Augmented Generation (RAG)** works with Large Language Models (LLMs) like Gemini.\n",
    "\n",
    "### What is RAG?\n",
    "RAG is a technique that combines:\n",
    "1. **Retrieval**: Fetch relevant documents/chunks from a knowledge base based on a query\n",
    "2. **Augmentation**: Provide those chunks as context to an LLM\n",
    "3. **Generation**: Let the LLM generate answers grounded in the retrieved context\n",
    "\n",
    "### Why use RAG?\n",
    "- **Reduces Hallucinations**: LLMs only answer based on provided documents\n",
    "- **Keeps Knowledge Fresh**: Update documents without retraining models\n",
    "- **Improves Factuality**: Answers are grounded in your specific data\n",
    "- **Explains Sources**: You can see which chunks contributed to the answer\n",
    "\n",
    "### In this notebook:\n",
    "- Build a minimal in-memory vector store\n",
    "- Implement semantic search using TF-IDF (+ keyword matching)\n",
    "- Use Gemini API to generate LLM-powered answers\n",
    "- See the complete RAG pipeline in action\n",
    "\n",
    "**Requirements:** A free Gemini API key from [Google AI Studio](https://aistudio.google.com/app/apikey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb47fcd",
   "metadata": {
    "id": "7cb47fcd"
   },
   "source": [
    "## Section 0: Setup & Dependencies\n",
    "\n",
    "This section imports necessary libraries and defines helper functions for text processing and vector operations.\n",
    "\n",
    "### Key Functions:\n",
    "- `normalize_text()`: Cleans text for consistent processing\n",
    "- `split_into_sentences()`: Breaks documents into sentences for chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20ec4c8a",
   "metadata": {
    "id": "20ec4c8a"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Imports and helpers\n",
    "import math, re, collections, os\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Optional: use TF-IDF from scikit-learn if available\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    SKLEARN_OK = True\n",
    "except Exception:\n",
    "    SKLEARN_OK = False\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s\\.]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    # Very small sentence splitter\n",
    "    parts = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    return [p.strip() for p in parts if p.strip()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498d8f96",
   "metadata": {
    "id": "498d8f96"
   },
   "source": [
    "## Section 1: Sample Knowledge Base (Corpus)\n",
    "\n",
    "We create a small collection of documents about **RAG, vector stores, and search techniques**.\n",
    "\n",
    "In a real application, this would be replaced with:\n",
    "- Company documentation\n",
    "- Research papers\n",
    "- Product catalogs\n",
    "- User manuals\n",
    "- etc.\n",
    "\n",
    "The RAG system will search this corpus to answer user questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c37acb55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c37acb55",
    "outputId": "fc4d6d82-fdcb-4d6f-9b81-daa19cdfeaec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1: RAG, or Retrieval-Augmented Generation, is a technique where we fetch relevant context fro...\n",
      "Doc 2: A vector store keeps representations (embeddings) of chunks so we can quickly find semanti...\n",
      "Doc 3: Chunking breaks long documents into smaller segments (like paragraphs or sliding windows)....\n",
      "Doc 4: Cosine similarity measures how close two vectors point in the same direction. Higher cosin...\n",
      "Doc 5: You can start with TF-IDF for quick experiments and later switch to stronger embeddings li...\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "'RAG, or Retrieval-Augmented Generation, is a technique where we fetch relevant context from a knowledge base and then generate answers grounded in that context. It reduces hallucinations and improves factuality.',\n",
    "'A vector store keeps representations (embeddings) of chunks so we can quickly find semantically similar pieces to a query.',\n",
    "'Chunking breaks long documents into smaller segments (like paragraphs or sliding windows). Good chunking helps retrieval match the right context.',\n",
    "'Cosine similarity measures how close two vectors point in the same direction. Higher cosine means more similar meanings.',\n",
    "'You can start with TF-IDF for quick experiments and later switch to stronger embeddings like Sentence-Transformers. For scale, tools like FAISS, Milvus, or Chroma are common.'\n",
    "]\n",
    "\n",
    "\n",
    "for i, doc in enumerate(corpus, 1):\n",
    "    print(f\"Doc {i}:\", doc[:90] + (\"...\" if len(doc) > 90 else \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc7e698",
   "metadata": {
    "id": "dcc7e698"
   },
   "source": [
    "## Section 2: Document Chunking\n",
    "\n",
    "### Why chunk documents?\n",
    "Long documents need to be split into smaller chunks for better retrieval:\n",
    "- **Semantic Precision**: Smaller chunks are more focused on single topics\n",
    "- **Better Matching**: Queries are more likely to match relevant sections\n",
    "- **Efficient Indexing**: Faster storage and retrieval in vector stores\n",
    "- **Context Windows**: LLM context limits require appropriately sized chunks\n",
    "\n",
    "### Chunking Strategy:\n",
    "We use a **sentence-based sliding window** approach:\n",
    "- Group N consecutive sentences into a chunk\n",
    "- Can be extended with overlapping windows for better context preservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "900edec2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "900edec2",
    "outputId": "e8cb9b3d-5520-4be9-f2f0-6919c3118712"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 5\n",
      "Chunk 1: RAG, or Retrieval-Augmented Generation, is a technique where we fetch relevant context from a knowledge base and then generate answers grounded in that context. It reduces hallucinations and improves factuality.\n",
      "Chunk 2: A vector store keeps representations (embeddings) of chunks so we can quickly find semantically similar pieces to a query.\n",
      "Chunk 3: Chunking breaks long documents into smaller segments (like paragraphs or sliding windows). Good chunking helps retrieval match the right context.\n",
      "Chunk 4: Cosine similarity measures how close two vectors point in the same direction. Higher cosine means more similar meanings.\n",
      "Chunk 5: You can start with TF-IDF for quick experiments and later switch to stronger embeddings like Sentence-Transformers. For scale, tools like FAISS, Milvus, or Chroma are common.\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "def chunk_docs(docs: List[str], sentences_per_chunk: int = 2) -> List[str]:\n",
    "    chunks = []\n",
    "    for doc in docs:\n",
    "        sents = split_into_sentences(doc)\n",
    "        for i in range(0, len(sents), sentences_per_chunk):\n",
    "            chunk = \" \".join(sents[i:i+sentences_per_chunk])\n",
    "            if chunk.strip():\n",
    "                chunks.append(chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_docs(corpus, sentences_per_chunk=2)\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "for i, c in enumerate(chunks[:5], 1):\n",
    "    print(f\"Chunk {i}: {c}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569e2c57",
   "metadata": {
    "id": "569e2c57"
   },
   "source": [
    "## Section 3: In-Memory Vector Store & Smart Retrieval\n",
    "\n",
    "### How the Vector Store Works:\n",
    "\n",
    "1. **Vectorization**: Convert text chunks into numerical vectors\n",
    "   - Uses **TF-IDF** (Term Frequency-Inverse Document Frequency) for weighted term importance\n",
    "   - Fallback to **Bag-of-Words** if scikit-learn unavailable\n",
    "\n",
    "2. **Vector Normalization**: Scale vectors to unit length for consistent similarity scores\n",
    "\n",
    "3. **Similarity Search**: Find chunks most similar to a query\n",
    "   - **Cosine Similarity**: Measures angle between vectors (0-1 range)\n",
    "   - **Keyword Bonus**: Extra boost for exact term matches\n",
    "   - **Combined Score**: Weighted blend of semantic + keyword relevance\n",
    "\n",
    "### Retrieval Pipeline:\n",
    "```\n",
    "User Query ‚Üí Vectorize ‚Üí Cosine Similarity Scores ‚Üí Keyword Bonus\n",
    "‚Üí Combined Ranking ‚Üí Return Top-K Chunks\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9893e04",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9893e04",
    "outputId": "86bc0209-2987-4b3e-f5a8-7cc8ffa7c61e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer method: tfidf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class TinyVectorStore:\n",
    "    def __init__(self, chunks: List[str]):\n",
    "        self.chunks = chunks\n",
    "        self.method = \"tfidf\" if SKLEARN_OK else \"bow_fallback\"\n",
    "        if SKLEARN_OK:\n",
    "            self.vectorizer = TfidfVectorizer(lowercase=True)\n",
    "            self.matrix = self.vectorizer.fit_transform(chunks)  # sparse matrix\n",
    "        else:\n",
    "            self._build_bow_index(chunks)\n",
    "\n",
    "    def _build_bow_index(self, chunks: List[str]):\n",
    "        tokenized = [normalize_text(c).split() for c in chunks]\n",
    "        df = collections.Counter()\n",
    "        for toks in tokenized:\n",
    "            df.update(set(toks))\n",
    "        N = len(chunks)\n",
    "        self.vocab, self.idf = {}, {}\n",
    "        for i, term in enumerate(sorted(df.keys())):\n",
    "            self.vocab[term] = i\n",
    "            self.idf[term] = math.log((1 + N) / (1 + df[term])) + 1.0\n",
    "        self.matrix = []\n",
    "        for toks in tokenized:\n",
    "            tf = collections.Counter(toks)\n",
    "            vec = [0.0] * len(self.vocab)\n",
    "            for t, cnt in tf.items():\n",
    "                if t in self.vocab:\n",
    "                    vec[self.vocab[t]] = (cnt / max(1,len(toks))) * self.idf[t]\n",
    "            norm = math.sqrt(sum(v*v for v in vec)) or 1.0\n",
    "            vec = [v / norm for v in vec]\n",
    "            self.matrix.append(vec)\n",
    "\n",
    "    def encode_query(self, query: str):\n",
    "        if SKLEARN_OK:\n",
    "            return self.vectorizer.transform([query])\n",
    "        else:\n",
    "            toks = normalize_text(query).split()\n",
    "            tf = collections.Counter(toks)\n",
    "            vec = [0.0] * len(self.vocab)\n",
    "            if len(toks) == 0:\n",
    "                return vec\n",
    "            for t, cnt in tf.items():\n",
    "                if t in self.vocab:\n",
    "                    vec[self.vocab[t]] = (cnt / len(toks)) * self.idf.get(t, 0.0)\n",
    "            norm = math.sqrt(sum(v*v for v in vec)) or 1.0\n",
    "            return [v / norm for v in vec]\n",
    "\n",
    "    def cosine_scores(self, qvec):\n",
    "        if SKLEARN_OK:\n",
    "            sims = (self.matrix @ qvec.T).toarray().ravel()\n",
    "            return sims.tolist()\n",
    "        else:\n",
    "            sims = []\n",
    "            for vec in self.matrix:\n",
    "                s = sum(a*b for a,b in zip(vec, qvec))\n",
    "                sims.append(s)\n",
    "            return sims\n",
    "\n",
    "    def search(self, query: str, k: int = 3, alpha: float = 0.85) -> List[Tuple[int, float, str]]:\n",
    "        qvec = self.encode_query(query)\n",
    "        sims = self.cosine_scores(qvec)\n",
    "        q_terms = set(normalize_text(query).split())\n",
    "        keyword_bonus = []\n",
    "        for chunk in self.chunks:\n",
    "            terms = set(normalize_text(chunk).split())\n",
    "            overlap = len(q_terms & terms) / max(1, len(q_terms))\n",
    "            keyword_bonus.append(overlap)\n",
    "        # Combine\n",
    "        scores = [alpha*s + (1-alpha)*b for s,b in zip(sims, keyword_bonus)]\n",
    "        tops = sorted(list(enumerate(scores)), key=lambda x: x[1], reverse=True)[:k]\n",
    "        return [(idx, scores[idx], self.chunks[idx]) for idx, _ in tops]\n",
    "\n",
    "store = TinyVectorStore(chunks)\n",
    "print(\"Vectorizer method:\", store.method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92b7c30",
   "metadata": {
    "id": "f92b7c30"
   },
   "source": [
    "## Section 4: Baseline Extractive Answer (Without LLM)\n",
    "\n",
    "### What is Extractive QA?\n",
    "Before using an LLM, let's see a baseline approach:\n",
    "- Retrieve relevant chunks using semantic search\n",
    "- Extract sentences that contain query terms\n",
    "- Combine sentences into an answer\n",
    "\n",
    "### Limitations of Extractive QA:\n",
    "- ‚ùå Cannot synthesize information across multiple chunks\n",
    "- ‚ùå Cannot reformulate or explain concepts\n",
    "- ‚ùå May include irrelevant sentences\n",
    "- ‚úÖ But: Fast, deterministic, and doesn't require an API\n",
    "\n",
    "### Next: We'll upgrade to LLM-powered generation for better answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14542f8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14542f8a",
    "outputId": "2b723d05-59d4-4bc0-99af-bc795c6387a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is RAG and why are vector stores useful?',\n",
       " 'retrieved': [(0,\n",
       "   0.29330802646365534,\n",
       "   'RAG, or Retrieval-Augmented Generation, is a technique where we fetch relevant context from a knowledge base and then generate answers grounded in that context. It reduces hallucinations and improves factuality.'),\n",
       "  (4,\n",
       "   0.15488700510938208,\n",
       "   'You can start with TF-IDF for quick experiments and later switch to stronger embeddings like Sentence-Transformers. For scale, tools like FAISS, Milvus, or Chroma are common.'),\n",
       "  (1,\n",
       "   0.11758022982737715,\n",
       "   'A vector store keeps representations (embeddings) of chunks so we can quickly find semantically similar pieces to a query.')],\n",
       " 'answer': 'RAG, or Retrieval-Augmented Generation, is a technique where we fetch relevant context from a knowledge base and then generate answers grounded in that context. It reduces hallucinations and improves factuality. You can start with TF-IDF for quick experiments and later switch to stronger embeddings like Sentence-Transformers.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def extractive_answer(query: str, retrieved: List[Tuple[int, float, str]], max_sents: int = 3) -> str:\n",
    "    q_terms = set(normalize_text(query).split())\n",
    "    chosen = []\n",
    "    for _, _, chunk in retrieved:\n",
    "        for s in split_into_sentences(chunk):\n",
    "            s_terms = set(normalize_text(s).split())\n",
    "            if len(q_terms & s_terms) > 0:\n",
    "                chosen.append(s)\n",
    "                if len(chosen) >= max_sents:\n",
    "                    break\n",
    "        if len(chosen) >= max_sents:\n",
    "            break\n",
    "    if not chosen:\n",
    "        for _, _, chunk in retrieved:\n",
    "            for s in split_into_sentences(chunk):\n",
    "                chosen.append(s)\n",
    "                if len(chosen) >= max_sents:\n",
    "                    break\n",
    "            if len(chosen) >= max_sents:\n",
    "                break\n",
    "    return \" \".join(chosen)\n",
    "\n",
    "def rag_ask_extractive(query: str, k: int = 3) -> Dict[str, object]:\n",
    "    retrieved = store.search(query, k=k)\n",
    "    answer = extractive_answer(query, retrieved, max_sents=3)\n",
    "    return {\"query\": query, \"retrieved\": retrieved, \"answer\": answer}\n",
    "\n",
    "demo = rag_ask_extractive(\"What is RAG and why are vector stores useful?\", k=3)\n",
    "demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c429d7",
   "metadata": {
    "id": "e4c429d7"
   },
   "source": [
    "## Section 5: Gemini API Setup\n",
    "\n",
    "### What is Gemini?\n",
    "**Google's Gemini** is a family of advanced multimodal LLMs that can:\n",
    "- Generate coherent, contextual text\n",
    "- Understand and reason about provided context\n",
    "- Follow instructions reliably\n",
    "\n",
    "### Available Gemini Models:\n",
    "- **gemini-1.5-flash**: Fast, efficient, great for most use cases ‚≠ê (we'll use this)\n",
    "- **gemini-1.5-pro**: More powerful, better reasoning (costs more)\n",
    "- **gemini-2.0-flash**: Latest and fastest (when available)\n",
    "\n",
    "### Setup Steps:\n",
    "1. Get a free API key: https://aistudio.google.com/app/apikey\n",
    "2. Set `GEMINI_API_KEY` environment variable\n",
    "3. Install `google-generativeai` SDK\n",
    "\n",
    "### Why use Gemini for RAG?\n",
    "- ‚úÖ Free tier available\n",
    "- ‚úÖ Fast inference\n",
    "- ‚úÖ Good context understanding\n",
    "- ‚úÖ Reliable instruction following\n",
    "- ‚úÖ Can be prompted to stay within retrieved context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fd0bf1a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fd0bf1a",
    "outputId": "20c4e475-8728-4240-dced-eb9142904d69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Gemini SDK is already installed.\n",
      "\n",
      "üîë Enter your Gemini API key below (input is hidden for security):\n",
      "‚úÖ API key set successfully in this session.\n",
      "\n",
      "=== Available Models ===\n",
      "‚úì models/gemini-2.5-pro-preview-03-25\n",
      "‚úì models/gemini-2.5-flash-preview-05-20\n",
      "‚úì models/gemini-2.5-flash\n",
      "‚úì models/gemini-2.5-flash-lite-preview-06-17\n",
      "‚úì models/gemini-2.5-pro-preview-05-06\n",
      "‚úì models/gemini-2.5-pro-preview-06-05\n",
      "‚úì models/gemini-2.5-pro\n",
      "‚úì models/gemini-2.0-flash-exp\n",
      "‚úì models/gemini-2.0-flash\n",
      "‚úì models/gemini-2.0-flash-001\n",
      "‚úì models/gemini-2.0-flash-exp-image-generation\n",
      "‚úì models/gemini-2.0-flash-lite-001\n",
      "‚úì models/gemini-2.0-flash-lite\n",
      "‚úì models/gemini-2.0-flash-preview-image-generation\n",
      "‚úì models/gemini-2.0-flash-lite-preview-02-05\n",
      "‚úì models/gemini-2.0-flash-lite-preview\n",
      "‚úì models/gemini-2.0-pro-exp\n",
      "‚úì models/gemini-2.0-pro-exp-02-05\n",
      "‚úì models/gemini-exp-1206\n",
      "‚úì models/gemini-2.0-flash-thinking-exp-01-21\n",
      "‚úì models/gemini-2.0-flash-thinking-exp\n",
      "‚úì models/gemini-2.0-flash-thinking-exp-1219\n",
      "‚úì models/gemini-2.5-flash-preview-tts\n",
      "‚úì models/gemini-2.5-pro-preview-tts\n",
      "‚úì models/learnlm-2.0-flash-experimental\n",
      "‚úì models/gemma-3-1b-it\n",
      "‚úì models/gemma-3-4b-it\n",
      "‚úì models/gemma-3-12b-it\n",
      "‚úì models/gemma-3-27b-it\n",
      "‚úì models/gemma-3n-e4b-it\n",
      "‚úì models/gemma-3n-e2b-it\n",
      "‚úì models/gemini-flash-latest\n",
      "‚úì models/gemini-flash-lite-latest\n",
      "‚úì models/gemini-pro-latest\n",
      "‚úì models/gemini-2.5-flash-lite\n",
      "‚úì models/gemini-2.5-flash-image-preview\n",
      "‚úì models/gemini-2.5-flash-image\n",
      "‚úì models/gemini-2.5-flash-preview-09-2025\n",
      "‚úì models/gemini-2.5-flash-lite-preview-09-2025\n",
      "‚úì models/gemini-robotics-er-1.5-preview\n",
      "‚úì models/gemini-2.5-computer-use-preview-10-2025\n",
      "\n",
      "Using model: models/gemini-2.0-flash-exp\n"
     ]
    }
   ],
   "source": [
    "# 5.1 Install Gemini SDK (if needed)\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "    print(\"\\n‚úÖ Gemini SDK is already installed.\")\n",
    "except Exception:\n",
    "    print(\"\\nInstalling google-generativeai SDK...\")\n",
    "    import sys, subprocess\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"google-generativeai\"], check=False)\n",
    "    import google.generativeai as genai\n",
    "    print(\"‚úÖ Gemini SDK installed.\")\n",
    "\n",
    "# 5.2 Configure your API key\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    print(\"\\nüîë Enter your Gemini API key below (input is hidden for security):\")\n",
    "    GEMINI_API_KEY = getpass(\"Gemini API Key: \")\n",
    "    if GEMINI_API_KEY:\n",
    "        os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY\n",
    "        print(\"‚úÖ API key set successfully in this session.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No API key entered. Please provide one to enable LLM calls.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ GEMINI_API_KEY already found in environment.\")\n",
    "\n",
    "# 5.3 Configure Gemini and list available models\n",
    "if GEMINI_API_KEY:\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    try:\n",
    "        print(\"\\n=== Available Models ===\")\n",
    "        for model in genai.list_models():\n",
    "            if 'generateContent' in model.supported_generation_methods:\n",
    "                print(f\"‚úì {model.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not list models: {e}\")\n",
    "\n",
    "# 5.3 Choose your model (use gemini-pro for text-only, or gemini-pro-vision for images)\n",
    "GEMINI_MODEL = \"models/gemini-2.0-flash-exp\"\n",
    "print(f\"\\nUsing model: {GEMINI_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb35a065",
   "metadata": {
    "id": "fb35a065"
   },
   "source": [
    "## Section 6: LLM-Powered RAG Answer Generation\n",
    "\n",
    "### The RAG Loop with Gemini:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  User Query     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ\n",
    "         ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 1. RETRIEVE              ‚îÇ  Vector store searches for\n",
    "‚îÇ    Semantic Search       ‚îÇ  relevant chunks\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ\n",
    "         ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 2. AUGMENT               ‚îÇ  Format retrieved chunks\n",
    "‚îÇ    Build Context         ‚îÇ  as prompt context\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ\n",
    "         ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 3. GENERATE              ‚îÇ  Send query + context\n",
    "‚îÇ    Gemini LLM            ‚îÇ  to Gemini API\n",
    "‚îÇ    (with instructions)   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ\n",
    "         ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  LLM-Generated Answer   ‚îÇ\n",
    "‚îÇ  (grounded in context)  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Key Benefits of LLM-Powered RAG:\n",
    "- üß† **Synthesis**: Combine information from multiple chunks\n",
    "- üìù **Explanation**: Generate coherent, natural language answers\n",
    "- üéØ **Accuracy**: System prompt ensures answers stay within context\n",
    "- üîç **Traceability**: Can show which chunks informed the answer\n",
    "\n",
    "### System Prompt Design:\n",
    "The system prompt is critical‚Äîit tells the LLM:\n",
    "- **What role to play** (helpful assistant)\n",
    "- **Scope constraints** (answer ONLY from provided context)\n",
    "- **Failure mode** (what to say if context doesn't contain the answer)\n",
    "\n",
    "This prevents the LLM from hallucinating information not in your knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d4761b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9d4761b9",
    "outputId": "01cd782f-5148-4d17-a067-5b65581f8955"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GEMINI LLM DEMO ===\n",
      "Query: Explain RAG and how cosine similarity helps retrieval.\n",
      "\n",
      "Context:\n",
      "[Chunk 1]\n",
      "Cosine similarity measures how close two vectors point in the same direction. Higher cosine means more similar meanings.\n",
      "\n",
      "[Chunk 2]\n",
      "RAG, or Retrieval-Augmented Generation, is a technique where we fetch relevant context from a knowledge base and then generate answers grounded in that context. It reduces hallucinations and improves factuality.\n",
      "\n",
      "[Chunk 3]\n",
      "Chunking breaks long documents into smaller segments (like paragraphs or sliding windows). Good chunking helps retrieval match the right context.\n",
      "\n",
      "Answer: RAG, or Retrieval-Augmented Generation, is a technique where we fetch relevant context from a knowledge base and then generate answers grounded in that context. Cosine similarity measures how close two vectors point in the same direction, where higher cosine means more similar meanings.\n"
     ]
    }
   ],
   "source": [
    "def build_context(chunks: List[str], max_chars: int = 2000) -> str:\n",
    "    ctx = \"\"\n",
    "    for i, ch in enumerate(chunks, 1):\n",
    "        block = f\"[Chunk {i}]\\n{ch}\\n\\n\"\n",
    "        if len(ctx) + len(block) > max_chars:\n",
    "            break\n",
    "        ctx += block\n",
    "    return ctx.strip()\n",
    "\n",
    "def call_gemini_chat(system: str, user: str, model: str = None, max_tokens: int = 400, temperature: float = 0.2) -> str:\n",
    "    model = model or GEMINI_MODEL\n",
    "    key = os.getenv(\"GEMINI_API_KEY\", \"\")\n",
    "    if not key:\n",
    "        return \"GEMINI_API_KEY not set. Please set it and re-run.\"\n",
    "    try:\n",
    "        import google.generativeai as genai\n",
    "        genai.configure(api_key=key)\n",
    "        \n",
    "        # Create the model\n",
    "        client = genai.GenerativeModel(model_name=model)\n",
    "        \n",
    "        # Build the full prompt with system instructions\n",
    "        full_prompt = f\"{system}\\n\\n{user}\"\n",
    "        \n",
    "        # Generate content\n",
    "        resp = client.generate_content(\n",
    "            full_prompt,\n",
    "            generation_config={\n",
    "                \"max_output_tokens\": max_tokens,\n",
    "                \"temperature\": temperature,\n",
    "            }\n",
    "        )\n",
    "        return resp.text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"[Gemini error] {e}\"\n",
    "\n",
    "def rag_ask_gemini(query: str, k: int = 3, max_ctx_chars: int = 2000, max_tokens: int = 400) -> Dict[str, object]:\n",
    "    retrieved = store.search(query, k=k)\n",
    "    top_texts = [t for _, _, t in retrieved]\n",
    "    context = build_context(top_texts, max_chars=max_ctx_chars)\n",
    "    system = (\n",
    "        \"You are a helpful assistant. Answer ONLY using the provided [Chunk] context. \"\n",
    "        \"If the answer is not in the context, say you don't have enough information.\"\n",
    "    )\n",
    "    user_prompt = f\"Question: {query}\\n\\nContext:\\n{context}\\n\\nAnswer:\"\n",
    "    answer = call_gemini_chat(system, user_prompt, model=GEMINI_MODEL, max_tokens=max_tokens, temperature=0.2)\n",
    "    return {\"query\": query, \"retrieved\": retrieved, \"context\": context, \"answer\": answer}\n",
    "\n",
    "# Demo call (will only work if GEMINI_API_KEY is configured)\n",
    "print(\"\\n=== GEMINI LLM DEMO ===\")\n",
    "demo_llm = rag_ask_gemini(\"Explain RAG and how cosine similarity helps retrieval.\", k=3)\n",
    "print(f\"Query: {demo_llm['query']}\")\n",
    "print(f\"\\nContext:\\n{demo_llm['context']}\")\n",
    "print(f\"\\nAnswer: {demo_llm['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea960ebe",
   "metadata": {
    "id": "ea960ebe"
   },
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "#### üîπ RAG Fundamentals\n",
    "- **Retrieval**: Efficient document search using vector similarity\n",
    "- **Augmentation**: Formatting retrieved context for LLMs\n",
    "- **Generation**: LLM-powered answer synthesis grounded in facts\n",
    "\n",
    "#### üîπ Technical Components\n",
    "- **Text normalization** for consistent processing\n",
    "- **Document chunking** for optimal retrieval\n",
    "- **TF-IDF vectorization** for semantic search\n",
    "- **Cosine similarity** for relevance scoring\n",
    "- **Keyword matching** for exact match boosting\n",
    "- **System prompts** to guide LLM behavior\n",
    "\n",
    "#### üîπ Integration\n",
    "- Using Gemini API for high-quality text generation\n",
    "- Controlling LLM behavior through careful prompting\n",
    "- Preventing hallucinations with context constraints\n",
    "\n",
    "### Practical Use Cases:\n",
    "‚úÖ Customer support chatbots (FAQ-based answering)\n",
    "‚úÖ Document Q&A systems (legal, medical, technical docs)\n",
    "‚úÖ Internal knowledge bases (company wikis, runbooks)\n",
    "‚úÖ Research paper analysis and summarization\n",
    "‚úÖ Codebase documentation and Q&A\n",
    "‚úÖ Product recommendation engines\n",
    "\n",
    "### Next Steps to Explore:\n",
    "\n",
    "**1. Improve Retrieval Quality**\n",
    "```python\n",
    "# Use better embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(chunks)\n",
    "```\n",
    "\n",
    "**2. Scale to Production**\n",
    "```python\n",
    "# Use vector database\n",
    "import faiss\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(embeddings)\n",
    "```\n",
    "\n",
    "**3. Add Reranking**\n",
    "```python\n",
    "# Rerank with cross-encoder\n",
    "from sentence_transformers import CrossEncoder\n",
    "reranker = CrossEncoder('cross-encoder/ms-marco-TinyBERT-L-2-v2')\n",
    "scores = reranker.predict([[query, doc] for doc in top_k_docs])\n",
    "```\n",
    "\n",
    "**4. Implement Streaming**\n",
    "```python\n",
    "# Stream Gemini responses\n",
    "for chunk in client.generate_content(..., stream=True):\n",
    "    print(chunk.text, end='', flush=True)\n",
    "```\n",
    "\n",
    "**5. Add Multi-Hop Reasoning**\n",
    "```python\n",
    "# Agent that can make multiple retrieval passes\n",
    "# (Build a CrewAI agent with retriever + reasoner tools)\n",
    "```\n",
    "\n",
    "### Resources:\n",
    "- üìñ [Google Gemini API Docs](https://ai.google.dev/docs)\n",
    "- üìñ [Retrieval Augmented Generation Survey](https://arxiv.org/abs/2312.10997)\n",
    "- üõ†Ô∏è [LangChain RAG Guide](https://python.langchain.com/docs/use_cases/question_answering/)\n",
    "- üõ†Ô∏è [LlamaIndex Documentation](https://docs.llamaindex.ai/)\n",
    "- üõ†Ô∏è [Sentence-Transformers](https://www.sbert.net/)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now understand how RAG works and can build your own systems. Happy building! üöÄ"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
