{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3895c05",
      "metadata": {},
      "source": [
        "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
        "\n",
        "# **LLMs Vs AI Agents**\n",
        "\n",
        "---\n",
        "\n",
        "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog/<BLOG_NAME>)\n",
        "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
        "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bab2309",
      "metadata": {
        "id": "2bab2309"
      },
      "source": [
        "Welcome to this hands-on notebook! In this walkthrough, we'll explore two essential building blocks of agentic AI using **local models**:\n",
        "\n",
        "1. **Simple LLM Inference**\n",
        "   Learn how to make a direct call to a local Large Language Model (LLM) using Ollama and generate a response.\n",
        "\n",
        "2. **Making Your First AI Agent**\n",
        "   Understand how an LLM can be extended with memory, tools, and behaviors using SmolAgents framework.\n",
        "\n",
        "This notebook is part of the *Build your AI Agents* series ‚Äî designed to break down complex concepts into simple, practical code examples using **local models** that run on your machine.\n",
        "\n",
        "**Prerequisites:**\n",
        "- Ollama installed and running (`ollama serve`)\n",
        "- Models downloaded (e.g., `ollama pull qwen2.5vl:7b`)\n",
        "\n",
        "![SmolAgents Logo](https://camo.githubusercontent.com/c6efa99360afde7cf829dff3cad81e56573658c1843464dff1fbb30a8f63b082/68747470733a2f2f68756767696e67666163652e636f2f64617461736574732f68756767696e67666163652f646f63756d656e746174696f6e2d696d616765732f7265736f6c76652f6d61696e2f736d6f6c6167656e74732f736d6f6c6167656e74732e706e67)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d743484",
      "metadata": {
        "id": "9d743484"
      },
      "source": [
        "## Part 1 ‚Äì Simple Local LLM Inference with Ollama\n",
        "\n",
        "Large language models (LLMs) are predictive text models trained on vast amounts of data. They generate responses by predicting the next word given a prompt. This section shows how to make a direct call to a **local LLM** running on Ollama.\n",
        "\n",
        "**Benefits of Local LLMs:**\n",
        "- üîí **Privacy** - Your data never leaves your machine\n",
        "- üí∞ **Cost** - No API charges, unlimited usage\n",
        "- ‚ö° **Speed** - No network latency\n",
        "- üéõÔ∏è **Control** - Full control over model parameters\n",
        "\n",
        "**Note:** Make sure Ollama is running (`ollama serve`) and you have models available (`ollama list`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2cec29ed",
      "metadata": {
        "id": "2cec29ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Ollama is running!\n",
            "üì¶ Available models: 4\n",
            "   1. qwen3:4b\n",
            "   2. qwen2.5vl:7b\n",
            "   3. llama3.2-vision:11b\n",
            "   4. llama3.2-vision:latest\n"
          ]
        }
      ],
      "source": [
        "# üì¶ Install required packages\n",
        "!pip install requests -q\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# üîç Check if Ollama is running and list available models\n",
        "def check_ollama_status():\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            models = response.json().get('models', [])\n",
        "            print(\"‚úÖ Ollama is running!\")\n",
        "            print(f\"üì¶ Available models: {len(models)}\")\n",
        "            \n",
        "            # Show available models\n",
        "            for i, model in enumerate(models[:5]):\n",
        "                print(f\"   {i+1}. {model['name']}\")\n",
        "            return [model['name'] for model in models]\n",
        "        else:\n",
        "            print(\"‚ùå Ollama not responding\")\n",
        "            return []\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Cannot connect to Ollama: {e}\")\n",
        "        print(\"   Please start Ollama: ollama serve\")\n",
        "        return []\n",
        "\n",
        "available_models = check_ollama_status()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "simple_llm_call",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚ú® Simple LLM call using Ollama API\n",
        "def call_ollama_model(prompt, model=\"qwen3:8b\"):\n",
        "    \"\"\"Make a direct call to Ollama model\"\"\"\n",
        "    url = \"http://localhost:11434/api/generate\"\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"prompt\": prompt,\n",
        "        \"stream\": False\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        response = requests.post(url, json=data, timeout=60)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()['response']\n",
        "        else:\n",
        "            return f\"Error: {response.status_code}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error calling Ollama: {str(e)}\"\n",
        "\n",
        "# Test simple LLM inference\n",
        "prompt = \"What is an AI agent? Explain in simple terms.\"\n",
        "response = call_ollama_model(prompt, \"qwen2.5vl:7b\")\n",
        "\n",
        "print(\"üìù Simple LLM Response:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93bad5c1",
      "metadata": {
        "id": "93bad5c1"
      },
      "source": [
        "In this code, we connect directly to Ollama's API endpoint, send a prompt to `qwen3:8b` (or your preferred model), and get back a single response.\n",
        "\n",
        "Simple LLM calls like this are **reactive**: they respond to the prompt but do not remember previous interactions or take any action beyond producing text.\n",
        "\n",
        "**Key differences from cloud APIs:**\n",
        "- No API keys required\n",
        "- Models run locally on your hardware\n",
        "- Response time depends on your machine's capabilities\n",
        "- Complete privacy and control over data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71e98911",
      "metadata": {
        "id": "71e98911"
      },
      "source": [
        "## Part 2 ‚Äì Building an AI Agent with SmolAgents\n",
        "\n",
        "You can create agents in many ways ‚Äî either **without a framework** (by wiring together an LLM with memory, tools, and control loops manually) or **with a framework** that handles these components for you.\n",
        "\n",
        "Here, we'll use **SmolAgents** from Hugging Face, which is designed to be:\n",
        "- üöÄ **Simple** - Only ~1,000 lines of core code\n",
        "- üßë‚Äçüíª **Code-first** - Agents write Python code to solve problems\n",
        "- üîß **Model-agnostic** - Works with any LLM (local or cloud)\n",
        "- üõ†Ô∏è **Tool-friendly** - Easy integration with external tools\n",
        "\n",
        "SmolAgents turns a plain LLM into an **agent** by augmenting it with:\n",
        "- üß† **Memory** ‚Äì keeps track of interactions through conversation history\n",
        "- üõ†Ô∏è **Tools** ‚Äì lets the agent execute Python code and use external APIs\n",
        "- ü§î **Reasoning** ‚Äì allows step-by-step problem solving with code\n",
        "\n",
        "**Why SmolAgents is perfect for local models:**\n",
        "- Works great with Ollama models\n",
        "- Lightweight and fast\n",
        "- No external dependencies for basic functionality\n",
        "- Code execution happens locally\n",
        "\n",
        "Let's install SmolAgents and create our first agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6ac2cfd3",
      "metadata": {
        "id": "6ac2cfd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ SmolAgents installed!\n",
            "ü§ñ Local model configured for SmolAgents!\n",
            "   Model: qwen3:4b\n",
            "   Endpoint: http://localhost:11434/v1\n",
            "   Running locally via Ollama\n"
          ]
        }
      ],
      "source": [
        "# üì¶ Install SmolAgents with toolkit\n",
        "!pip install \"smolagents[toolkit]\" -q\n",
        "!pip install openai -q\n",
        "\n",
        "\n",
        "from smolagents import CodeAgent, OpenAIServerModel\n",
        "\n",
        "print(\"‚úÖ SmolAgents installed!\")\n",
        "\n",
        "# ü§ñ Create Ollama model for SmolAgents\n",
        "# SmolAgents uses OpenAI-compatible API, which Ollama provides\n",
        "model = OpenAIServerModel(\n",
        "    model_id=\"qwen3:4b\",  # Your local model\n",
        "    api_base=\"http://localhost:11434/v1\",  # Ollama's OpenAI-compatible endpoint\n",
        "    api_key=\"ollama\",  # Ollama doesn't need a real API key\n",
        "    temperature=0.1  # Lower temperature for more consistent responses\n",
        ")\n",
        "\n",
        "print(\"ü§ñ Local model configured for SmolAgents!\")\n",
        "print(f\"   Model: qwen3:4b\")\n",
        "print(f\"   Endpoint: http://localhost:11434/v1\")\n",
        "print(f\"   Running locally via Ollama\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "create_basic_agent",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Agent with web search created!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span> <span style=\"font-weight: bold\">What are the main differences between a pure LLM and an AI agent?</span>                                               <span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚ï∞‚îÄ OpenAIServerModel - qwen3:4b ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[38;2;212;183;2m‚ï≠‚îÄ\u001b[0m\u001b[38;2;212;183;2m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[38;2;212;183;2m‚îÄ‚ïÆ\u001b[0m\n",
              "\u001b[38;2;212;183;2m‚îÇ\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m‚îÇ\u001b[0m\n",
              "\u001b[38;2;212;183;2m‚îÇ\u001b[0m \u001b[1mWhat are the main differences between a pure LLM and an AI agent?\u001b[0m                                               \u001b[38;2;212;183;2m‚îÇ\u001b[0m\n",
              "\u001b[38;2;212;183;2m‚îÇ\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m‚îÇ\u001b[0m\n",
              "\u001b[38;2;212;183;2m‚ï∞‚îÄ\u001b[0m\u001b[38;2;212;183;2m OpenAIServerModel - qwen3:4b \u001b[0m\u001b[38;2;212;183;2m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[38;2;212;183;2m‚îÄ‚ïØ\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ </span><span style=\"font-weight: bold\">Step 1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[38;2;212;183;2m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ \u001b[0m\u001b[1mStep 1\u001b[0m\u001b[38;2;212;183;2m ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ‚îÄ <span style=\"font-weight: bold\">Executing parsed code:</span> ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">result </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> web_search(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"main differences between pure LLM and AI agent\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                    </span>  \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(result)</span><span style=\"background-color: #272822\">                                                                                                  </span>  \n",
              " ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n",
              "</pre>\n"
            ],
            "text/plain": [
              " ‚îÄ \u001b[1mExecuting parsed code:\u001b[0m ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mresult\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mweb_search\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mmain differences between pure LLM and AI agent\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                    \u001b[0m  \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresult\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                  \u001b[0m  \n",
              " ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
              "## Search Results\n",
              "\n",
              "[AI Agents Vs LLMs: Understanding The Differences &amp; ...](https://acecloud.ai/blog/ai-agents-vs-llms/)\n",
              "Jul 3, 2025 ‚Äî Unlike LLMs, AI agents don't generate text ‚Äì they execute tasks. They are designed to work \n",
              "autonomously, making decisions based on real world ...\n",
              "\n",
              "[Understanding the Differences Between LLM, RAG, and AI \n",
              "...](https://medium.com/@hannanorris591/understanding-the-differences-between-llm-rag-and-ai-agents-for-businesses-\n",
              "1196da15c513)\n",
              "Unlike LLMs that generate answers or RAG systems that provide accurate insights, AI Agents combine intelligence \n",
              "with action . They can understand ...\n",
              "\n",
              "[Unpacking the Power of AI Agents vs. Pure \n",
              "LLMs](https://www.dsstream.com/post/beyond-text-generation-unpacking-the-power-of-ai-agents-vs-pure-llms)\n",
              "The shift from pure LLMs to AI agents represents a pivotal step in AI evolution. Both have their place, but they \n",
              "serve different purposes.\n",
              "\n",
              "[LLMs to AI Agents: What's the Difference and Why It \n",
              "Matters](https://digitalworkforce.com/rpa-news/live-webinar-from-llms-to-ai-agents-whats-the-difference-and-why-it-\n",
              "matters/)\n",
              "Why enterprise-grade agents are more reliable, stable, and auditable than pure LLM solutions. The role of \n",
              "orchestration, tool use, and context in real-world ...\n",
              "\n",
              "[Beyond Text Generation: Understanding the Power of AI Agents \n",
              "...](https://medium.com/@ds_stream/beyond-text-generation-understanding-the-power-of-ai-agents-vs-pure-llms-0ec8981\n",
              "f80b1)\n",
              "AI agents, by contrast, build upon LLMs but extend far beyond text generation. They maintain state across \n",
              "interactions, integrate with external tools and APIs, ...\n",
              "\n",
              "[AI Agent vs LLM (Large Language Model)](https://bito.ai/blog/ai-agent-vs-llm/)\n",
              "Mar 13, 2025 ‚Äî The key distinction remains: if the AI is just generating content (no matter how impressive), it's \n",
              "acting as an LLM ; if it's deciding and doing ...\n",
              "\n",
              "[AI Agents vs. AI Assistants](https://www.ibm.com/think/topics/ai-agents-vs-ai-assistants)\n",
              "Decision-making and action: The ability to call on tools by itself does not make an LLM an agent . AI agents can \n",
              "also act autonomously and decide which tools to ...\n",
              "\n",
              "[If you're unsure what Agentic AI is and what's the difference \n",
              "...](https://www.reddit.com/r/AI_Agents/comments/1hqzzrg/if_youre_unsure_what_agentic_ai_is_and_whats_the/)\n",
              "The key distinction between AI agents and workflows actually lies in the autonomy of decision-making. AI agents can\n",
              "independently determine ...\n",
              "\n",
              "[LLM ‚Üí RAG ‚Üí AI Workflow ‚Üí AI \n",
              "Agent](https://www.codelink.io/blog/post/ai-system-development-llm-rag-ai-workflow-agent)\n",
              "A pure LLM is essentially a lossy compression of the internet, a snapshot of knowledge from its training data. It \n",
              "excels at tasks involving this stored ...\n",
              "\n",
              "[Why and How to Build AI Agents for LLM Applications \n",
              "-](https://deepsense.ai/blog/why-and-how-to-build-ai-agents-for-llm-applications/)\n",
              "Differences Between AI Copilots, Chatbots, and Autonomous Agents , and Decision Recommendation Systems ... and \n",
              "limitations becomes easier after ...\n",
              "\n",
              "Out: None\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mExecution logs:\u001b[0m\n",
              "## Search Results\n",
              "\n",
              "[AI Agents Vs LLMs: Understanding The Differences & ...](https://acecloud.ai/blog/ai-agents-vs-llms/)\n",
              "Jul 3, 2025 ‚Äî Unlike LLMs, AI agents don't generate text ‚Äì they execute tasks. They are designed to work \n",
              "autonomously, making decisions based on real world ...\n",
              "\n",
              "[Understanding the Differences Between LLM, RAG, and AI \n",
              "...](https://medium.com/@hannanorris591/understanding-the-differences-between-llm-rag-and-ai-agents-for-businesses-\n",
              "1196da15c513)\n",
              "Unlike LLMs that generate answers or RAG systems that provide accurate insights, AI Agents combine intelligence \n",
              "with action . They can understand ...\n",
              "\n",
              "[Unpacking the Power of AI Agents vs. Pure \n",
              "LLMs](https://www.dsstream.com/post/beyond-text-generation-unpacking-the-power-of-ai-agents-vs-pure-llms)\n",
              "The shift from pure LLMs to AI agents represents a pivotal step in AI evolution. Both have their place, but they \n",
              "serve different purposes.\n",
              "\n",
              "[LLMs to AI Agents: What's the Difference and Why It \n",
              "Matters](https://digitalworkforce.com/rpa-news/live-webinar-from-llms-to-ai-agents-whats-the-difference-and-why-it-\n",
              "matters/)\n",
              "Why enterprise-grade agents are more reliable, stable, and auditable than pure LLM solutions. The role of \n",
              "orchestration, tool use, and context in real-world ...\n",
              "\n",
              "[Beyond Text Generation: Understanding the Power of AI Agents \n",
              "...](https://medium.com/@ds_stream/beyond-text-generation-understanding-the-power-of-ai-agents-vs-pure-llms-0ec8981\n",
              "f80b1)\n",
              "AI agents, by contrast, build upon LLMs but extend far beyond text generation. They maintain state across \n",
              "interactions, integrate with external tools and APIs, ...\n",
              "\n",
              "[AI Agent vs LLM (Large Language Model)](https://bito.ai/blog/ai-agent-vs-llm/)\n",
              "Mar 13, 2025 ‚Äî The key distinction remains: if the AI is just generating content (no matter how impressive), it's \n",
              "acting as an LLM ; if it's deciding and doing ...\n",
              "\n",
              "[AI Agents vs. AI Assistants](https://www.ibm.com/think/topics/ai-agents-vs-ai-assistants)\n",
              "Decision-making and action: The ability to call on tools by itself does not make an LLM an agent . AI agents can \n",
              "also act autonomously and decide which tools to ...\n",
              "\n",
              "[If you're unsure what Agentic AI is and what's the difference \n",
              "...](https://www.reddit.com/r/AI_Agents/comments/1hqzzrg/if_youre_unsure_what_agentic_ai_is_and_whats_the/)\n",
              "The key distinction between AI agents and workflows actually lies in the autonomy of decision-making. AI agents can\n",
              "independently determine ...\n",
              "\n",
              "[LLM ‚Üí RAG ‚Üí AI Workflow ‚Üí AI \n",
              "Agent](https://www.codelink.io/blog/post/ai-system-development-llm-rag-ai-workflow-agent)\n",
              "A pure LLM is essentially a lossy compression of the internet, a snapshot of knowledge from its training data. It \n",
              "excels at tasks involving this stored ...\n",
              "\n",
              "[Why and How to Build AI Agents for LLM Applications \n",
              "-](https://deepsense.ai/blog/why-and-how-to-build-ai-agents-for-llm-applications/)\n",
              "Differences Between AI Copilots, Chatbots, and Autonomous Agents , and Decision Recommendation Systems ... and \n",
              "limitations becomes easier after ...\n",
              "\n",
              "Out: None\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 22.98 seconds| Input tokens: 2,020 | Output tokens: 661]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2m[Step 1: Duration 22.98 seconds| Input tokens: 2,020 | Output tokens: 661]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ </span><span style=\"font-weight: bold\">Step 2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[38;2;212;183;2m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ \u001b[0m\u001b[1mStep 2\u001b[0m\u001b[38;2;212;183;2m ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Error in code parsing:</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Your code snippet is invalid, because the regex pattern &lt;code&gt;(.*?)&lt;/code&gt; was not found in it.</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">                Here is your code snippet:</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">                &lt;think&gt;</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Okay, let's tackle this question: \"What are the main differences between a pure LLM and an AI agent?\"</span>\n",
              "\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">First, I need to figure out what the user is asking. They want to know the key differences between a pure Large </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Language Model (LLM) and an AI agent. From the search results provided, I can see that there are several articles </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">discussing this topic.</span>\n",
              "\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Looking at the search results, the main points seem to be:</span>\n",
              "\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">1. **Functionality**: Pure LLMs generate text, while AI agents execute tasks autonomously.</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">2. **Autonomy**: AI agents can make decisions and take actions without human intervention, whereas LLMs just </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">respond to prompts.</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">3. **State Maintenance**: AI agents maintain state across interactions, while LLMs don't have memory between </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">sessions.</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">4. **Tool Integration**: AI agents can integrate with external tools and APIs, whereas LLMs don't do that by </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">default.</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">5. **Action vs. Generation**: LLMs generate text, but agents perform actions (like calling APIs, making decisions).</span>\n",
              "\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">The user probably expects a concise list of the main differences. Let me check the search results again to make </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">sure I'm not missing anything.</span>\n",
              "\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">From the first result: \"Unlike LLMs, AI agents don't generate text ‚Äì they execute tasks. They are designed to work </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">autonomously, making decisions based on real world...\"</span>\n",
              "\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Second result: \"AI Agents combine intelligence with action. They can understand...\"</span>\n",
              "\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Third result: \"The shift from pure LLMs to AI agents represents a pivotal step in AI evolution. Both have their </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">place, but they serve different purposes.\"</span>\n",
              "\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Fourth result: \"Why enterprise-grade agents are more reliable, stable, and auditable than pure LLM solutions. The </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">role of orchestration, tool use, and context in real-world...\"</span>\n",
              "\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Fifth result: \"AI agents, by contrast, build upon LLMs but extend far beyond text generation. They maintain state </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">across interactions, integrate with external tools and APIs...\"</span>\n",
              "\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">So the key differences are:</span>\n",
              "\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">1. **Text Generation vs. Task Execution**: LLMs generate text; agents execute tasks.</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">2. **Autonomy**: Agents work autonomously, making decisions and taking actions without human input.</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">3. **State Maintenance**: Agents maintain context and state across interactions, while LLMs don't.</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">4. **Tool Integration**: Agents can interact with external tools and APIs, whereas LLMs are limited to their </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">training data.</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">5. **Actionability**: Agents can perform actions (like web searches, API calls), while LLMs can only generate text.</span>\n",
              "\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">I need to present these differences clearly. Since the user asked for the main differences, I should list them </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">concisely.</span>\n",
              "\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Wait, the user's task is to answer the question using the tools provided. But in this case, the tools available are</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">`web_search` and `final_answer`. The previous examples show that when you need to get information, you use </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">`web_search` to get results, then process them.</span>\n",
              "\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">In the previous steps, the assistant did a web search and then used the results to answer. Here, the user has </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">already provided the search results (the Observation logs). So the next step is to analyze those results and </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">formulate the answer.</span>\n",
              "\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Wait, the problem says: \"New task: What are the main differences between a pure LLM and an AI agent?\" and then </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">there's an Observation section with the search results.</span>\n",
              "\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Wait, looking back at the problem statement: The user provided the task, and then the Observation section shows the</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">search results from a previous code execution. Wait, no‚Äîthe Observation here is part of the problem setup. Wait, </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">no, in the problem description, the user says: \"New task: ...\", and then the Observation section shows the search </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">results from a web search.</span>\n",
              "\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Wait, in the problem setup, the user has given the task and the Observation logs (the search results). So the </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">assistant needs to process those search results to answer the question.</span>\n",
              "\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Wait, the problem says: \"&lt;/code&gt;</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">                It seems like you're trying to return the final answer, you can do it as follows:</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">                &lt;code&gt;</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">                final_answer(\"YOUR FINAL ANSWER HERE\")</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">                &lt;/code&gt;</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Make sure to provide correct code blobs.</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;31mError in code parsing:\u001b[0m\n",
              "\u001b[1;31mYour code snippet is invalid, because the regex pattern <code>(.*?)</code> was not found in it.\u001b[0m\n",
              "\u001b[1;31m                Here is your code snippet:\u001b[0m\n",
              "\u001b[1;31m                <think>\u001b[0m\n",
              "\u001b[1;31mOkay, let's tackle this question: \"What are the main differences between a pure LLM and an AI agent?\"\u001b[0m\n",
              "\n",
              "\u001b[1;31mFirst, I need to figure out what the user is asking. They want to know the key differences between a pure Large \u001b[0m\n",
              "\u001b[1;31mLanguage Model (LLM) and an AI agent. From the search results provided, I can see that there are several articles \u001b[0m\n",
              "\u001b[1;31mdiscussing this topic.\u001b[0m\n",
              "\n",
              "\u001b[1;31mLooking at the search results, the main points seem to be:\u001b[0m\n",
              "\n",
              "\u001b[1;31m1. **Functionality**: Pure LLMs generate text, while AI agents execute tasks autonomously.\u001b[0m\n",
              "\u001b[1;31m2. **Autonomy**: AI agents can make decisions and take actions without human intervention, whereas LLMs just \u001b[0m\n",
              "\u001b[1;31mrespond to prompts.\u001b[0m\n",
              "\u001b[1;31m3. **State Maintenance**: AI agents maintain state across interactions, while LLMs don't have memory between \u001b[0m\n",
              "\u001b[1;31msessions.\u001b[0m\n",
              "\u001b[1;31m4. **Tool Integration**: AI agents can integrate with external tools and APIs, whereas LLMs don't do that by \u001b[0m\n",
              "\u001b[1;31mdefault.\u001b[0m\n",
              "\u001b[1;31m5. **Action vs. Generation**: LLMs generate text, but agents perform actions (like calling APIs, making decisions).\u001b[0m\n",
              "\n",
              "\u001b[1;31mThe user probably expects a concise list of the main differences. Let me check the search results again to make \u001b[0m\n",
              "\u001b[1;31msure I'm not missing anything.\u001b[0m\n",
              "\n",
              "\u001b[1;31mFrom the first result: \"Unlike LLMs, AI agents don't generate text ‚Äì they execute tasks. They are designed to work \u001b[0m\n",
              "\u001b[1;31mautonomously, making decisions based on real world...\"\u001b[0m\n",
              "\n",
              "\u001b[1;31mSecond result: \"AI Agents combine intelligence with action. They can understand...\"\u001b[0m\n",
              "\n",
              "\u001b[1;31mThird result: \"The shift from pure LLMs to AI agents represents a pivotal step in AI evolution. Both have their \u001b[0m\n",
              "\u001b[1;31mplace, but they serve different purposes.\"\u001b[0m\n",
              "\n",
              "\u001b[1;31mFourth result: \"Why enterprise-grade agents are more reliable, stable, and auditable than pure LLM solutions. The \u001b[0m\n",
              "\u001b[1;31mrole of orchestration, tool use, and context in real-world...\"\u001b[0m\n",
              "\n",
              "\u001b[1;31mFifth result: \"AI agents, by contrast, build upon LLMs but extend far beyond text generation. They maintain state \u001b[0m\n",
              "\u001b[1;31macross interactions, integrate with external tools and APIs...\"\u001b[0m\n",
              "\n",
              "\u001b[1;31mSo the key differences are:\u001b[0m\n",
              "\n",
              "\u001b[1;31m1. **Text Generation vs. Task Execution**: LLMs generate text; agents execute tasks.\u001b[0m\n",
              "\u001b[1;31m2. **Autonomy**: Agents work autonomously, making decisions and taking actions without human input.\u001b[0m\n",
              "\u001b[1;31m3. **State Maintenance**: Agents maintain context and state across interactions, while LLMs don't.\u001b[0m\n",
              "\u001b[1;31m4. **Tool Integration**: Agents can interact with external tools and APIs, whereas LLMs are limited to their \u001b[0m\n",
              "\u001b[1;31mtraining data.\u001b[0m\n",
              "\u001b[1;31m5. **Actionability**: Agents can perform actions (like web searches, API calls), while LLMs can only generate text.\u001b[0m\n",
              "\n",
              "\u001b[1;31mI need to present these differences clearly. Since the user asked for the main differences, I should list them \u001b[0m\n",
              "\u001b[1;31mconcisely.\u001b[0m\n",
              "\n",
              "\u001b[1;31mWait, the user's task is to answer the question using the tools provided. But in this case, the tools available are\u001b[0m\n",
              "\u001b[1;31m`web_search` and `final_answer`. The previous examples show that when you need to get information, you use \u001b[0m\n",
              "\u001b[1;31m`web_search` to get results, then process them.\u001b[0m\n",
              "\n",
              "\u001b[1;31mIn the previous steps, the assistant did a web search and then used the results to answer. Here, the user has \u001b[0m\n",
              "\u001b[1;31malready provided the search results (the Observation logs). So the next step is to analyze those results and \u001b[0m\n",
              "\u001b[1;31mformulate the answer.\u001b[0m\n",
              "\n",
              "\u001b[1;31mWait, the problem says: \"New task: What are the main differences between a pure LLM and an AI agent?\" and then \u001b[0m\n",
              "\u001b[1;31mthere's an Observation section with the search results.\u001b[0m\n",
              "\n",
              "\u001b[1;31mWait, looking back at the problem statement: The user provided the task, and then the Observation section shows the\u001b[0m\n",
              "\u001b[1;31msearch results from a previous code execution. Wait, no‚Äîthe Observation here is part of the problem setup. Wait, \u001b[0m\n",
              "\u001b[1;31mno, in the problem description, the user says: \"New task: ...\", and then the Observation section shows the search \u001b[0m\n",
              "\u001b[1;31mresults from a web search.\u001b[0m\n",
              "\n",
              "\u001b[1;31mWait, in the problem setup, the user has given the task and the Observation logs (the search results). So the \u001b[0m\n",
              "\u001b[1;31massistant needs to process those search results to answer the question.\u001b[0m\n",
              "\n",
              "\u001b[1;31mWait, the problem says: \"</code>\u001b[0m\n",
              "\u001b[1;31m                It seems like you're trying to return the final answer, you can do it as follows:\u001b[0m\n",
              "\u001b[1;31m                <code>\u001b[0m\n",
              "\u001b[1;31m                final_answer(\"YOUR FINAL ANSWER HERE\")\u001b[0m\n",
              "\u001b[1;31m                </code>\u001b[0m\n",
              "\u001b[1;31mMake sure to provide correct code blobs.\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 20.43 seconds| Input tokens: 4,801 | Output tokens: 1,477]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2m[Step 2: Duration 20.43 seconds| Input tokens: 4,801 | Output tokens: 1,477]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ </span><span style=\"font-weight: bold\">Step 3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[38;2;212;183;2m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ \u001b[0m\u001b[1mStep 3\u001b[0m\u001b[38;2;212;183;2m ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ‚îÄ <span style=\"font-weight: bold\">Executing parsed code:</span> ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">(</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.*</span><span style=\"color: #ed007e; text-decoration-color: #ed007e; background-color: #1e0010\">?</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                                          </span>  \n",
              " ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n",
              "</pre>\n"
            ],
            "text/plain": [
              " ‚îÄ \u001b[1mExecuting parsed code:\u001b[0m ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m*\u001b[0m\u001b[38;2;237;0;126;48;2;30;0;16m?\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                          \u001b[0m  \n",
              " ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
              "## Search Results\n",
              "\n",
              "[AI Agents Vs LLMs: Understanding The Differences &amp; ...](https://acecloud.ai/blog/ai-agents-vs-llms/)\n",
              "Jul 3, 2025 ‚Äî Unlike LLMs, AI agents don't generate text ‚Äì they execute tasks. They are designed to work \n",
              "autonomously, making decisions based on real world ...\n",
              "\n",
              "[Understanding the Differences Between LLM, RAG, and AI \n",
              "...](https://medium.com/@hannanorris591/understanding-the-differences-between-llm-rag-and-ai-agents-for-businesses-\n",
              "1196da15c513)\n",
              "Unlike LLMs that generate answers or RAG systems that provide accurate insights, AI Agents combine intelligence \n",
              "with action . They can understand ...\n",
              "\n",
              "[Unpacking the Power of AI Agents vs. Pure \n",
              "LLMs](https://www.dsstream.com/post/beyond-text-generation-unpacking-the-power-of-ai-agents-vs-pure-llms)\n",
              "The shift from pure LLMs to AI agents represents a pivotal step in AI evolution. Both have their place, but they \n",
              "serve different purposes.\n",
              "\n",
              "[LLMs to AI Agents: What's the Difference and Why It \n",
              "Matters](https://digitalworkforce.com/rpa-news/live-webinar-from-llms-to-ai-agents-whats-the-difference-and-why-it-\n",
              "matters/)\n",
              "Why enterprise-grade agents are more reliable, stable, and auditable than pure LLM solutions. The role of \n",
              "orchestration, tool use, and context in real-world ...\n",
              "\n",
              "[Beyond Text Generation: Understanding the Power of AI Agents \n",
              "...](https://medium.com/@ds_stream/beyond-text-generation-understanding-the-power-of-ai-agents-vs-pure-llms-0ec8981\n",
              "f80b1)\n",
              "AI agents, by contrast, build upon LLMs but extend far beyond text generation. They maintain state across \n",
              "interactions, integrate with external tools and APIs, ...\n",
              "\n",
              "[AI Agent vs LLM (Large Language Model)](https://bito.ai/blog/ai-agent-vs-llm/)\n",
              "Mar 13, 2025 ‚Äî The key distinction remains: if the AI is just generating content (no matter how impressive), it's \n",
              "acting as an LLM ; if it's deciding and doing ...\n",
              "\n",
              "[AI Agents vs. AI Assistants](https://www.ibm.com/think/topics/ai-agents-vs-ai-assistants)\n",
              "Decision-making and action: The ability to call on tools by itself does not make an LLM an agent . AI agents can \n",
              "also act autonomously and decide which tools to ...\n",
              "\n",
              "[If you're unsure what Agentic AI is and what's the difference \n",
              "...](https://www.reddit.com/r/AI_Agents/comments/1hqzzrg/if_youre_unsure_what_agentic_ai_is_and_whats_the/)\n",
              "The key distinction between AI agents and workflows actually lies in the autonomy of decision-making. AI agents can\n",
              "independently determine ...\n",
              "\n",
              "[LLM ‚Üí RAG ‚Üí AI Workflow ‚Üí AI \n",
              "Agent](https://www.codelink.io/blog/post/ai-system-development-llm-rag-ai-workflow-agent)\n",
              "A pure LLM is essentially a lossy compression of the internet, a snapshot of knowledge from its training data. It \n",
              "excels at tasks involving this stored ...\n",
              "\n",
              "[Why and How to Build AI Agents for LLM Applications \n",
              "-](https://deepsense.ai/blog/why-and-how-to-build-ai-agents-for-llm-applications/)\n",
              "Differences Between AI Copilots, Chatbots, and Autonomous Agents , and Decision Recommendation Systems ... and \n",
              "limitations becomes easier after ...\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mExecution logs:\u001b[0m\n",
              "## Search Results\n",
              "\n",
              "[AI Agents Vs LLMs: Understanding The Differences & ...](https://acecloud.ai/blog/ai-agents-vs-llms/)\n",
              "Jul 3, 2025 ‚Äî Unlike LLMs, AI agents don't generate text ‚Äì they execute tasks. They are designed to work \n",
              "autonomously, making decisions based on real world ...\n",
              "\n",
              "[Understanding the Differences Between LLM, RAG, and AI \n",
              "...](https://medium.com/@hannanorris591/understanding-the-differences-between-llm-rag-and-ai-agents-for-businesses-\n",
              "1196da15c513)\n",
              "Unlike LLMs that generate answers or RAG systems that provide accurate insights, AI Agents combine intelligence \n",
              "with action . They can understand ...\n",
              "\n",
              "[Unpacking the Power of AI Agents vs. Pure \n",
              "LLMs](https://www.dsstream.com/post/beyond-text-generation-unpacking-the-power-of-ai-agents-vs-pure-llms)\n",
              "The shift from pure LLMs to AI agents represents a pivotal step in AI evolution. Both have their place, but they \n",
              "serve different purposes.\n",
              "\n",
              "[LLMs to AI Agents: What's the Difference and Why It \n",
              "Matters](https://digitalworkforce.com/rpa-news/live-webinar-from-llms-to-ai-agents-whats-the-difference-and-why-it-\n",
              "matters/)\n",
              "Why enterprise-grade agents are more reliable, stable, and auditable than pure LLM solutions. The role of \n",
              "orchestration, tool use, and context in real-world ...\n",
              "\n",
              "[Beyond Text Generation: Understanding the Power of AI Agents \n",
              "...](https://medium.com/@ds_stream/beyond-text-generation-understanding-the-power-of-ai-agents-vs-pure-llms-0ec8981\n",
              "f80b1)\n",
              "AI agents, by contrast, build upon LLMs but extend far beyond text generation. They maintain state across \n",
              "interactions, integrate with external tools and APIs, ...\n",
              "\n",
              "[AI Agent vs LLM (Large Language Model)](https://bito.ai/blog/ai-agent-vs-llm/)\n",
              "Mar 13, 2025 ‚Äî The key distinction remains: if the AI is just generating content (no matter how impressive), it's \n",
              "acting as an LLM ; if it's deciding and doing ...\n",
              "\n",
              "[AI Agents vs. AI Assistants](https://www.ibm.com/think/topics/ai-agents-vs-ai-assistants)\n",
              "Decision-making and action: The ability to call on tools by itself does not make an LLM an agent . AI agents can \n",
              "also act autonomously and decide which tools to ...\n",
              "\n",
              "[If you're unsure what Agentic AI is and what's the difference \n",
              "...](https://www.reddit.com/r/AI_Agents/comments/1hqzzrg/if_youre_unsure_what_agentic_ai_is_and_whats_the/)\n",
              "The key distinction between AI agents and workflows actually lies in the autonomy of decision-making. AI agents can\n",
              "independently determine ...\n",
              "\n",
              "[LLM ‚Üí RAG ‚Üí AI Workflow ‚Üí AI \n",
              "Agent](https://www.codelink.io/blog/post/ai-system-development-llm-rag-ai-workflow-agent)\n",
              "A pure LLM is essentially a lossy compression of the internet, a snapshot of knowledge from its training data. It \n",
              "excels at tasks involving this stored ...\n",
              "\n",
              "[Why and How to Build AI Agents for LLM Applications \n",
              "-](https://deepsense.ai/blog/why-and-how-to-build-ai-agents-for-llm-applications/)\n",
              "Differences Between AI Copilots, Chatbots, and Autonomous Agents , and Decision Recommendation Systems ... and \n",
              "limitations becomes easier after ...\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Code parsing failed on line 1 due to: SyntaxError</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">(.*?)</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">  ^</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Error: invalid syntax (&lt;unknown&gt;, line 1)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;31mCode parsing failed on line 1 due to: SyntaxError\u001b[0m\n",
              "\u001b[1;31m(.*?)\u001b[0m\n",
              "\u001b[1;31m  ^\u001b[0m\n",
              "\u001b[1;31mError: invalid syntax (<unknown>, line 1)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 15.67 seconds| Input tokens: 8,518 | Output tokens: 2,072]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2m[Step 3: Duration 15.67 seconds| Input tokens: 8,518 | Output tokens: 2,072]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ </span><span style=\"font-weight: bold\">Step 4</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[38;2;212;183;2m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ \u001b[0m\u001b[1mStep 4\u001b[0m\u001b[38;2;212;183;2m ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ‚îÄ <span style=\"font-weight: bold\">Executing parsed code:</span> ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"The main differences between a pure LLM and an AI agent are: 1. Pure LLMs generate text </span><span style=\"background-color: #272822\">         </span>  \n",
              "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">responses, while AI agents execute tasks autonomously. 2. AI agents maintain state across interactions, whereas</span>  \n",
              "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">LLMs lack memory between sessions. 3. AI agents integrate with external tools and APIs, while LLMs are limited </span>  \n",
              "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">to their training data.\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                      </span>  \n",
              " ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n",
              "</pre>\n"
            ],
            "text/plain": [
              " ‚îÄ \u001b[1mExecuting parsed code:\u001b[0m ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mThe main differences between a pure LLM and an AI agent are: 1. Pure LLMs generate text \u001b[0m\u001b[48;2;39;40;34m         \u001b[0m  \n",
              "  \u001b[38;2;230;219;116;48;2;39;40;34mresponses, while AI agents execute tasks autonomously. 2. AI agents maintain state across interactions, whereas\u001b[0m  \n",
              "  \u001b[38;2;230;219;116;48;2;39;40;34mLLMs lack memory between sessions. 3. AI agents integrate with external tools and APIs, while LLMs are limited \u001b[0m  \n",
              "  \u001b[38;2;230;219;116;48;2;39;40;34mto their training data.\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                      \u001b[0m  \n",
              " ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The main differences between a pure LLM and an AI agent are: 1. Pure LLMs generate text responses, </span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">while AI agents execute tasks autonomously. 2. AI agents maintain state across interactions, whereas LLMs lack </span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">memory between sessions. 3. AI agents integrate with external tools and APIs, while LLMs are limited to their </span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">training data.</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;212;183;2mFinal answer: The main differences between a pure LLM and an AI agent are: 1. Pure LLMs generate text responses, \u001b[0m\n",
              "\u001b[1;38;2;212;183;2mwhile AI agents execute tasks autonomously. 2. AI agents maintain state across interactions, whereas LLMs lack \u001b[0m\n",
              "\u001b[1;38;2;212;183;2mmemory between sessions. 3. AI agents integrate with external tools and APIs, while LLMs are limited to their \u001b[0m\n",
              "\u001b[1;38;2;212;183;2mtraining data.\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 4: Duration 41.40 seconds| Input tokens: 12,270 | Output tokens: 3,655]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2m[Step 4: Duration 41.40 seconds| Input tokens: 12,270 | Output tokens: 3,655]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üåê Agent with Search Response:\n",
            "The main differences between a pure LLM and an AI agent are: 1. Pure LLMs generate text responses, while AI agents execute tasks autonomously. 2. AI agents maintain state across interactions, whereas LLMs lack memory between sessions. 3. AI agents integrate with external tools and APIs, while LLMs are limited to their training data.\n"
          ]
        }
      ],
      "source": [
        "# üéØ Create a basic SmolAgent\n",
        "from smolagents import DuckDuckGoSearchTool\n",
        "    \n",
        "agent_with_search = CodeAgent(\n",
        "    tools=[DuckDuckGoSearchTool()],  # Now web search is available\n",
        "    model=model,\n",
        "    instructions=\"You are a helpful assistant. You can search the web when needed to provide accurate information.\"\n",
        ")\n",
        "\n",
        "print(\"üîç Agent with web search created!\")\n",
        "question = \"What are the main differences between a pure LLM and an AI agent?\"\n",
        "response = agent_with_search.run(question)\n",
        "print(\"\\nüåê Agent with Search Response:\")\n",
        "print(response)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17279cda",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "finetune-yolo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
