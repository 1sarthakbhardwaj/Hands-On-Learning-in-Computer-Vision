{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a44c088",
   "metadata": {},
   "source": [
    "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
    "\n",
    "# **Fine-Tune YOLO for Traffic Flow Counting**\n",
    "\n",
    "---\n",
    "\n",
    "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog/<BLOG_NAME>)\n",
    "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
    "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)\n",
    "[![Scientific Paper](https://img.shields.io/badge/Official-Paper-blue.svg)](<PAPER LINK>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef38fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import random\n",
    "from ultralytics import YOLO\n",
    "from collections import defaultdict, deque\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e016976d",
   "metadata": {},
   "source": [
    "## **Plotting Region to Track Object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ffc27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons = []\n",
    "current_polygon = []\n",
    "\n",
    "def draw_polygon(event, x, y, flags, param):\n",
    "    global current_polygon\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        current_polygon.append((x, y))\n",
    "    elif event == cv2.EVENT_RBUTTONDOWN:\n",
    "        if len(current_polygon) > 2:\n",
    "            polygons.append(current_polygon.copy())\n",
    "        current_polygon = []\n",
    "\n",
    "def create_polygons_fullscreen(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return []\n",
    "    \n",
    "    _, frame = cap.read()  # Read single frame to get video resolution\n",
    "    if frame is None:\n",
    "        print(\"Error: Could not read video frame.\")\n",
    "        return []\n",
    "\n",
    "    cv2.namedWindow('Create Polygons', cv2.WINDOW_NORMAL)\n",
    "    cv2.setWindowProperty('Create Polygons', cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "    cv2.setMouseCallback('Create Polygons', draw_polygon)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Loop video\n",
    "            continue\n",
    "\n",
    "        # Draw existing polygons in green\n",
    "        for poly in polygons:\n",
    "            pts = np.array(poly, np.int32).reshape((-1, 1, 2))\n",
    "            cv2.polylines(frame, [pts], True, (0, 255, 0), 2)\n",
    "\n",
    "        # Draw current polygon being drawn in red\n",
    "        if len(current_polygon) > 1:\n",
    "            pts = np.array(current_polygon, np.int32).reshape((-1, 1, 2))\n",
    "            cv2.polylines(frame, [pts], False, (0, 0, 255), 2)\n",
    "\n",
    "        cv2.putText(frame, 'Left click: Add point', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 4)\n",
    "        cv2.putText(frame, 'Right click: Close polygon', (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 4)\n",
    "        cv2.putText(frame, 'Press Q to Quit', (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 4)\n",
    "\n",
    "        cv2.imshow('Create Polygons', frame)\n",
    "        key = cv2.waitKey(20) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return polygons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cbf3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons = create_polygons_fullscreen('assests/1.mp4')\n",
    "print(polygons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065df409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_polygons_on_video_matplotlib(video_path, polygons):\n",
    "    # Read the first frame from the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if not ret:\n",
    "        print(\"Failed to read video frame.\")\n",
    "        return\n",
    "\n",
    "    # Convert BGR to RGB for matplotlib\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.imshow(frame_rgb)\n",
    "\n",
    "    # Draw polygons in different colors\n",
    "    for poly in polygons:\n",
    "        # Generate a random color for each polygon, with 0.5 alpha\n",
    "        color = [random.random(), random.random(), random.random(), 0.5]\n",
    "        patch = patches.Polygon(poly, closed=True, facecolor=color, edgecolor=color[:3], linewidth=2)\n",
    "        ax.add_patch(patch)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26c75e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "polygons_1 = [\n",
    "    [(687, 870), (834, 729), (1437, 732), (1455, 1374), (102, 1329)],\n",
    "    [(1707, 720), (2118, 732), (2274, 723), (3204, 1191), (2085, 1230)]\n",
    "]\n",
    "show_polygons_on_video_matplotlib('assests/1.mp4', polygons_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77267f03",
   "metadata": {},
   "source": [
    "## **Counting the vehicle which passed through the region**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e5ef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VehicleCounter:\n",
    "    def __init__(self, polygons, model_path=None, target_classes=None):\n",
    "        \"\"\"\n",
    "        Initialize the vehicle counter with polygon regions\n",
    "        \n",
    "        Args:\n",
    "            polygons: List of polygon regions as [(x1,y1), (x2,y2), ...]\n",
    "            model_path: Path to custom YOLO model (optional)\n",
    "            target_classes: Dict of class_id: class_name for custom models (optional)\n",
    "        \"\"\"\n",
    "        self.polygons = polygons\n",
    "        self.region_counters = [0] * len(polygons)\n",
    "        self.region_colors = self._generate_region_colors()\n",
    "        self.tracked_objects = {}  # track_id: {last_region: int, history: deque}\n",
    "        \n",
    "        # Set up model and target classes\n",
    "        self.model_path = model_path\n",
    "        self._setup_model_and_classes(model_path, target_classes)\n",
    "        \n",
    "    def _setup_model_and_classes(self, model_path, target_classes):\n",
    "        \"\"\"Setup YOLO model and target classes\"\"\"\n",
    "        if model_path is None:\n",
    "            # Default: Use pre-trained YOLO model with COCO classes\n",
    "            self.model = YOLO('yolov8x.pt')\n",
    "            self.target_classes = {\n",
    "                2: 'car',\n",
    "                5: 'bus', \n",
    "                7: 'truck'\n",
    "            }\n",
    "            print(\"Using default YOLOv8x model with COCO dataset classes\")\n",
    "        else:\n",
    "            # Custom model\n",
    "            if not os.path.exists(model_path):\n",
    "                raise FileNotFoundError(f\"Custom model file not found: {model_path}\")\n",
    "            \n",
    "            self.model = YOLO(model_path)\n",
    "            \n",
    "            if target_classes is None:\n",
    "                # Try to get class names from model\n",
    "                if hasattr(self.model.model, 'names'):\n",
    "                    model_names = self.model.model.names\n",
    "                    self.target_classes = {i: name for i, name in model_names.items()}\n",
    "                    print(f\"Using all classes from custom model: {list(self.target_classes.values())}\")\n",
    "                else:\n",
    "                    # Fallback: assume single class or ask user to provide\n",
    "                    print(\"Warning: Could not determine class names from custom model.\")\n",
    "                    print(\"Assuming single class 'object'. Consider providing target_classes parameter.\")\n",
    "                    self.target_classes = {0: 'object'}\n",
    "            else:\n",
    "                # Use provided target classes\n",
    "                self.target_classes = target_classes\n",
    "                print(f\"Using custom model with specified classes: {list(target_classes.values())}\")\n",
    "            \n",
    "            print(f\"Loaded custom model from: {model_path}\")\n",
    "        \n",
    "    def _generate_region_colors(self):\n",
    "        \"\"\"Generate unique colors for each polygon region\"\"\"\n",
    "        color_names = ['red', 'green', 'blue', 'yellow', 'purple', 'orange', 'cyan', 'magenta', 'gray', 'pink', 'dark orange', 'teal', 'deep pink', 'hot pink', 'red orange']\n",
    "        colors = [\n",
    "            (0, 0, 255),    # red\n",
    "            (0, 255, 0),    # green\n",
    "            (255, 0, 0),    # blue\n",
    "            (0, 255, 255),  # yellow\n",
    "            (128, 0, 128),  # purple\n",
    "            (0, 165, 255),  # orange\n",
    "            (255, 255, 0),  # cyan\n",
    "            (255, 0, 255),   # magenta\n",
    "            (128, 128, 128),  # gray\n",
    "            (255, 192, 203), # pink\n",
    "            (255, 140, 0),   # dark orange\n",
    "            (0, 128, 128),   # teal\n",
    "            (255, 20, 147),  # deep pink\n",
    "            (255, 105, 180), # hot pink\n",
    "            (255, 69, 0)    # red orange\n",
    "        ]\n",
    "        \n",
    "        region_colors = []\n",
    "        for i in range(len(self.polygons)):\n",
    "            color_idx = i % len(colors)\n",
    "            region_colors.append({\n",
    "                'color': colors[color_idx],\n",
    "                'name': color_names[color_idx] if i < len(color_names) else f'region_{i}'\n",
    "            })\n",
    "        \n",
    "        return region_colors\n",
    "    \n",
    "    def _point_in_polygon(self, point, polygon):\n",
    "        \"\"\"Check if a point is inside a polygon using ray casting algorithm\"\"\"\n",
    "        x, y = point\n",
    "        n = len(polygon)\n",
    "        inside = False\n",
    "        \n",
    "        p1x, p1y = polygon[0]\n",
    "        for i in range(1, n + 1):\n",
    "            p2x, p2y = polygon[i % n]\n",
    "            if y > min(p1y, p2y):\n",
    "                if y <= max(p1y, p2y):\n",
    "                    if x <= max(p1x, p2x):\n",
    "                        if p1y != p2y:\n",
    "                            xinters = (y - p1y) * (p2x - p1x) / (p2y - p1y) + p1x\n",
    "                        if p1x == p2x or x <= xinters:\n",
    "                            inside = not inside\n",
    "            p1x, p1y = p2x, p2y\n",
    "        \n",
    "        return inside\n",
    "    \n",
    "    def _get_object_region(self, center_point):\n",
    "        \"\"\"Determine which region (if any) contains the object center point\"\"\"\n",
    "        for region_idx, polygon in enumerate(self.polygons):\n",
    "            if self._point_in_polygon(center_point, polygon):\n",
    "                return region_idx\n",
    "        return -1  # Not in any region\n",
    "    \n",
    "    def _draw_polygons(self, frame):\n",
    "        \"\"\"Draw polygon regions on the frame with colors and counters\"\"\"\n",
    "        overlay = frame.copy()\n",
    "        \n",
    "        for i, polygon in enumerate(self.polygons):\n",
    "            # Convert polygon to numpy array for OpenCV\n",
    "            pts = np.array(polygon, np.int32)\n",
    "            pts = pts.reshape((-1, 1, 2))\n",
    "            \n",
    "            # Draw filled polygon with transparency\n",
    "            cv2.fillPoly(overlay, [pts], self.region_colors[i]['color'])\n",
    "            \n",
    "            # Draw polygon outline\n",
    "            cv2.polylines(frame, [pts], True, self.region_colors[i]['color'], 3)\n",
    "            \n",
    "            # Calculate centroid for text placement\n",
    "            moments = cv2.moments(pts)\n",
    "            if moments['m00'] != 0:\n",
    "                cx = int(moments['m10'] / moments['m00'])\n",
    "                cy = int(moments['m01'] / moments['m00'])\n",
    "            else:\n",
    "                cx, cy = polygon[0]  # fallback to first point\n",
    "            \n",
    "            # Draw region info at centroid\n",
    "            region_text = f\"{self.region_colors[i]['name']}: {self.region_counters[i]}\"\n",
    "            text_size = cv2.getTextSize(region_text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)[0]\n",
    "            \n",
    "            # Background rectangle for text at centroid\n",
    "            cv2.rectangle(frame, \n",
    "                        (cx - text_size[0]//2 - 5, cy - text_size[1] - 5),\n",
    "                        (cx + text_size[0]//2 + 5, cy + 5),\n",
    "                        (0, 0, 0), -1)\n",
    "            \n",
    "            # Text at centroid\n",
    "            cv2.putText(frame, region_text, \n",
    "                    (cx - text_size[0]//2, cy),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "        \n",
    "        # Blend overlay with original frame for transparency effect\n",
    "        cv2.addWeighted(overlay, 0.3, frame, 0.7, 0, frame)\n",
    "        \n",
    "        # Draw region information on top right side\n",
    "        frame_height, frame_width = frame.shape[:2]\n",
    "        font_scale = 1.0  \n",
    "        font_thickness = 2\n",
    "        padding = 20\n",
    "        line_spacing = 100\n",
    "        \n",
    "        # Start position for top right display\n",
    "        start_y = padding + 90  \n",
    "        \n",
    "        for i, polygon in enumerate(self.polygons):\n",
    "            region_text = f\"{self.region_colors[i]['name']}: {self.region_counters[i]}\"\n",
    "            text_size = cv2.getTextSize(region_text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)[0]\n",
    "            text_width, text_height = text_size\n",
    "            \n",
    "            # Calculate position (right aligned with padding)\n",
    "            text_x = frame_width - text_width - padding\n",
    "            text_y = start_y + (i * line_spacing)\n",
    "            \n",
    "            # Draw background rectangle for better visibility\n",
    "            rect_padding = 15  \n",
    "            cv2.rectangle(frame,\n",
    "                        (text_x - rect_padding, text_y - text_height - rect_padding),\n",
    "                        (text_x + text_width + rect_padding, text_y + rect_padding),\n",
    "                        (0, 0, 0), -1)\n",
    "            \n",
    "            # Draw colored indicator bar next to text\n",
    "            indicator_width = 24  \n",
    "            cv2.rectangle(frame,\n",
    "                        (text_x - rect_padding - indicator_width - 15, text_y - text_height - rect_padding),\n",
    "                        (text_x - rect_padding - 15, text_y + rect_padding),\n",
    "                        self.region_colors[i]['color'], -1)\n",
    "            \n",
    "            # Draw the text\n",
    "            cv2.putText(frame, region_text,\n",
    "                    (text_x, text_y),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), font_thickness)\n",
    "        \n",
    "        return frame\n",
    "    \n",
    "    def _update_tracking(self, track_id, current_region):\n",
    "        \"\"\"Update tracking information and count if object crosses into new region\"\"\"\n",
    "        if track_id not in self.tracked_objects:\n",
    "            self.tracked_objects[track_id] = {\n",
    "                'last_region': current_region,\n",
    "                'history': deque(maxlen=10)  # Keep last 10 region positions\n",
    "            }\n",
    "        \n",
    "        obj_data = self.tracked_objects[track_id]\n",
    "        obj_data['history'].append(current_region)\n",
    "        \n",
    "        # Check if object moved from outside/different region into current region\n",
    "        if (obj_data['last_region'] != current_region and \n",
    "            current_region != -1 and \n",
    "            len(obj_data['history']) >= 2):\n",
    "            \n",
    "            # Count only if object was previously outside this region or in a different region\n",
    "            if obj_data['last_region'] != current_region:\n",
    "                self.region_counters[current_region] += 1\n",
    "                print(f\"Object {track_id} entered {self.region_colors[current_region]['name']} region. Count: {self.region_counters[current_region]}\")\n",
    "        \n",
    "        obj_data['last_region'] = current_region\n",
    "    \n",
    "    def process_video(self, video_path, output_path, confidence_threshold=0.2):\n",
    "        \"\"\"\n",
    "        Process video and save with region counting\n",
    "        \n",
    "        Args:\n",
    "            video_path: Path to input video\n",
    "            output_path: Path to save output video\n",
    "            confidence_threshold: Minimum confidence threshold for detections\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Error opening video file: {video_path}\")\n",
    "        \n",
    "        # Get video properties\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # Setup video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        print(f\"Processing video: {video_path}\")\n",
    "        print(f\"Output will be saved to: {output_path}\")\n",
    "        print(f\"Video properties: {width}x{height}, {fps} FPS, {total_frames} frames\")\n",
    "        print(f\"Using model: {self.model_path if self.model_path else 'YOLOv8x (default)'}\")\n",
    "        print(f\"Target classes: {list(self.target_classes.values())}\")\n",
    "        \n",
    "        frame_count = 0\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                frame_count += 1\n",
    "                \n",
    "                # Run YOLO tracking on the frame\n",
    "                # For custom models, we might need to specify classes or use all classes\n",
    "                if self.model_path is None:\n",
    "                    # Default model - specify target classes\n",
    "                    results = self.model.track(frame, persist=True, classes=list(self.target_classes.keys()))\n",
    "                else:\n",
    "                    # Custom model - use all classes or specified ones\n",
    "                    results = self.model.track(frame, persist=True)\n",
    "                \n",
    "                if results[0].boxes is not None and results[0].boxes.id is not None:\n",
    "                    # Get detections\n",
    "                    boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "                    track_ids = results[0].boxes.id.cpu().numpy().astype(int)\n",
    "                    classes = results[0].boxes.cls.cpu().numpy().astype(int)\n",
    "                    confidences = results[0].boxes.conf.cpu().numpy()\n",
    "                    \n",
    "                    # Process each detection\n",
    "                    for box, track_id, cls, conf in zip(boxes, track_ids, classes, confidences):\n",
    "                        # Check if this class is in our target classes and meets confidence threshold\n",
    "                        if cls in self.target_classes and conf > confidence_threshold:\n",
    "                            # Get center point of bounding box\n",
    "                            x1, y1, x2, y2 = box\n",
    "                            center_x = int((x1 + x2) / 2)\n",
    "                            center_y = int((y1 + y2) / 2)\n",
    "                            center_point = (center_x, center_y)\n",
    "                            \n",
    "                            # Determine which region the object is in\n",
    "                            current_region = self._get_object_region(center_point)\n",
    "                            \n",
    "                            # Update tracking and counting\n",
    "                            self._update_tracking(track_id, current_region)\n",
    "                            \n",
    "                            # Draw bounding box and label\n",
    "                            color = (0, 255, 0)  # Green for detections\n",
    "                            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "                            \n",
    "                            class_name = self.target_classes.get(cls, f'class_{cls}')\n",
    "                            label = f\"{class_name} ID:{track_id} {conf:.2f}\"\n",
    "                            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n",
    "                            cv2.rectangle(frame, (int(x1), int(y1) - label_size[1] - 5), \n",
    "                                        (int(x1) + label_size[0], int(y1)), color, -1)\n",
    "                            cv2.putText(frame, label, (int(x1), int(y1) - 5),\n",
    "                                      cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "                            \n",
    "                            # Draw center point\n",
    "                            cv2.circle(frame, center_point, 3, (0, 0, 255), -1)\n",
    "                \n",
    "                # Draw polygon regions and counters\n",
    "                frame = self._draw_polygons(frame)\n",
    "                \n",
    "                # Add frame info\n",
    "                info_text = f\"Frame: {frame_count}/{total_frames}\"\n",
    "                cv2.putText(frame, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "                \n",
    "                # Write frame to output video\n",
    "                out.write(frame)\n",
    "                \n",
    "                # Progress update\n",
    "                if frame_count % 100 == 0:\n",
    "                    print(f\"Processed {frame_count}/{total_frames} frames\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error during processing: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            # Cleanup\n",
    "            cap.release()\n",
    "            out.release()\n",
    "            \n",
    "        print(\"Processing completed!\")\n",
    "        print(\"Final counts per region:\")\n",
    "        for i, count in enumerate(self.region_counters):\n",
    "            print(f\"  {self.region_colors[i]['name']} region: {count}\")\n",
    "\n",
    "def count_vehicles_in_regions(polygons, video_path, output_path=\"output_with_counting.mp4\", \n",
    "                            model_path=None, target_classes=None, confidence_threshold=0.2):\n",
    "    \"\"\"\n",
    "    Main function to count vehicles passing through polygon regions\n",
    "    \n",
    "    Args:\n",
    "        polygons: List of polygon regions as [[(x1,y1), (x2,y2), ...], ...]\n",
    "        video_path: Path to input video file\n",
    "        output_path: Path to save output video (default: \"output_with_counting.mp4\")\n",
    "        model_path: Path to custom YOLO model (optional, uses YOLOv8x if None)\n",
    "        target_classes: Dict of class_id: class_name for custom models (optional)\n",
    "        confidence_threshold: Minimum confidence threshold for detections (default: 0.2)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Final counts for each region\n",
    "    \n",
    "    Examples:\n",
    "        # Using default YOLO model\n",
    "        counts = count_vehicles_in_regions(polygons, \"input_video.mp4\")\n",
    "        \n",
    "        # Using custom YOLO model with all classes\n",
    "        counts = count_vehicles_in_regions(polygons, \"input_video.mp4\", \n",
    "                                         model_path=\"my_custom_model.pt\")\n",
    "        \n",
    "        # Using custom YOLO model with specific target classes\n",
    "        custom_classes = {0: 'vehicle', 1: 'person', 2: 'bicycle'}\n",
    "        counts = count_vehicles_in_regions(polygons, \"input_video.mp4\", \n",
    "                                         model_path=\"my_custom_model.pt\",\n",
    "                                         target_classes=custom_classes)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create vehicle counter instance with custom model support\n",
    "        counter = VehicleCounter(polygons, model_path=model_path, target_classes=target_classes)\n",
    "        \n",
    "        # Process the video\n",
    "        counter.process_video(video_path, output_path, confidence_threshold=confidence_threshold)\n",
    "        \n",
    "        # Return final counts\n",
    "        final_counts = {}\n",
    "        for i, count in enumerate(counter.region_counters):\n",
    "            region_name = counter.region_colors[i]['name']\n",
    "            final_counts[region_name] = count\n",
    "        \n",
    "        return final_counts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in count_vehicles_in_regions: {e}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110d6b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using default YOLO model\n",
    "# polygon regions\n",
    "polygons = polygons_1\n",
    "\n",
    "counts1 = count_vehicles_in_regions(\n",
    "    polygons=polygons,\n",
    "    video_path=\"./assests/1.mp4\",\n",
    "    output_path=\"1_result.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeee2586",
   "metadata": {},
   "source": [
    "## **Custom Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13939b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Labellerr/yolo_finetune_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd8f40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo_finetune_utils.coco_yolo_converter.bbox_converter import coco_to_yolo_converter\n",
    "\n",
    "result = coco_to_yolo_converter(\n",
    "            json_path=r'./annotation.json',\n",
    "            images_dir=r'./dataset',\n",
    "            output_dir='yolo_format',\n",
    "            use_split=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccca1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=train data=\"./yolo_format/dataset.yaml\" model=\"yolov8x.pt\" epochs=200 imgsz=640 batch=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063f95b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=track model=\"./runs/detect/train/weights/last.pt\" source=\"./assests/3.mp4\" conf=0.25 save=True show_labels=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f575776",
   "metadata": {},
   "source": [
    "## **Counting Cars using Drone View Camera**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d892766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define polygon regions\n",
    "polygons2 = [\n",
    "            [(93, 322), (483, 260), (486, 483), (111, 522)],\n",
    "            [(112, 549), (482, 560), (507, 776), (123, 724)],\n",
    "            [(1443, 346), (1479, 564), (1870, 584), (1857, 430)],\n",
    "            [(1478, 598), (1414, 824), (1869, 784), (1868, 598)],\n",
    "            ]\n",
    "\n",
    "show_polygons_on_video_matplotlib('assests/3.mp4', polygons2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d495864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using custom YOLO model\n",
    "counts2 = count_vehicles_in_regions(\n",
    "    polygons=polygons,\n",
    "    video_path= \"./assests/3.mp4\",\n",
    "    output_path= \"3_result_custom_2.mp4\",\n",
    "    model_path= \"./runs/detect/train/weights/last.pt\",\n",
    "    target_classes= {0: 'car'},  # Optional: specify which classes to track\n",
    "    confidence_threshold=0.2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune-yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
