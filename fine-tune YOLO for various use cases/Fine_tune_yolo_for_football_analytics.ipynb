{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db785940",
   "metadata": {},
   "source": [
    "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
    "\n",
    "# **Fine-Tune YOLO for Football Player Tracking and Heatmap Generation**\n",
    "\n",
    "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog/)\n",
    "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
    "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)\n",
    "\n",
    "## Objective\n",
    "This notebook provides a complete workflow for building a football analytics tool using computer vision. We will fine-tune a **YOLO (You Only Look Once)** model to detect and track players on a football pitch. Using this model, we will generate advanced analytics like **player trajectories** and **positional heatmaps**.\n",
    "\n",
    "## Key Features\n",
    "* **Data Preparation**: Convert annotations from COCO format to the YOLO format required for training.\n",
    "* **Model Training**: Fine-tune a pre-trained YOLO model on a custom football dataset.\n",
    "* **Player Tracking**: Apply the trained model to track individual players across video frames.\n",
    "* **Trajectory Visualization**: Draw the paths of players to analyze movement patterns.\n",
    "* **Heatmap Generation**: Create heatmaps to visualize team positioning and field control.\n",
    "\n",
    "## Libraries & Prerequisites\n",
    "* **Core Libraries**: `ultralytics`, `opencv-python`, `matplotlib`, `numpy`.\n",
    "* **Environment**: A Python environment with GPU support (like Google Colab) is highly recommended for efficient model training.\n",
    "* **Dataset**: You'll need a custom dataset of football images with annotations in COCO format (`.json` file) and the corresponding video files for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d44b82",
   "metadata": {},
   "source": [
    "## **Environment Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0ddb51",
   "metadata": {},
   "source": [
    "First, let's set up our environment by cloning the utility repository, which contains the helper function to convert our dataset annotations from COCO to YOLO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e7f96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the utility repository to access the required functions\n",
    "!git clone https://github.com/yashsuman15/yolo_finetune_utils.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b841bcf4",
   "metadata": {},
   "source": [
    "## **Dataset Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e7eb33",
   "metadata": {},
   "source": [
    "This step prepares our dataset for training. The key function we'll use is `coco_to_yolo_converter`, which transforms annotations from the common COCO format into the specific YOLO `.txt` format required by the `ultralytics` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b75096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo_finetune_utils.coco_yolo_converter.bbox_converter import coco_to_yolo_converter\n",
    "# Convert COCO annotations to YOLO format\n",
    "# Ensure the paths are correct and the dataset_annotation.json is in COCO format\n",
    "# The images_dir should contain the images dataset\n",
    "# The json_path should point to the COCO annotations file\n",
    "result = coco_to_yolo_converter(\n",
    "            json_path='./dataset_annotation.json',\n",
    "            images_dir='./dataset',\n",
    "            output_dir='yolo_format',\n",
    "            use_split=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3853399",
   "metadata": {},
   "source": [
    "## **Training the YOLO Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e846997f",
   "metadata": {},
   "source": [
    "With our dataset properly formatted, we can now fine-tune the YOLO model. We'll start by checking the `ultralytics` installation. Then, we define the path to our dataset's configuration file (`dataset.yaml`) and launch the training process using a pre-trained model. This leverages **transfer learning** to adapt the model to our specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa2ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e4f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de35ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32823e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = !pwd\n",
    "dataset_path = f\"{location[0]}/yolo_format\"\n",
    "print(f\"Dataset path: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f9c8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=train data={dataset_path}/dataset.yaml model=\"yolo11x.pt\" epochs=200 imgsz=640 batch=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4623298e",
   "metadata": {},
   "source": [
    "## **Tracking Player on the Field**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6254ff7",
   "metadata": {},
   "source": [
    "Now that our model is trained, we can use it for tracking players in videos. We'll load our custom-trained weights and apply them to new video files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df111513",
   "metadata": {},
   "source": [
    "To quickly verify that our model's tracking is working, let's test it on a single video frame. We will load our new weights, run the tracker, and use `matplotlib` to visualize the bounding boxes and track IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfd4d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('./runs/detect/train2/weights/last.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b4046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.track(source=\"./Video/3.mp4\", persist=True, stream=True)\n",
    "\n",
    "# Find frame #30\n",
    "for frame_idx, res in enumerate(results):\n",
    "    if frame_idx < 30:\n",
    "        continue\n",
    "\n",
    "    # Grab img, boxes, track-IDs and class-IDs\n",
    "    frame_rgb = cv2.cvtColor(res.orig_img, cv2.COLOR_BGR2RGB)\n",
    "    boxes     = res.boxes.xyxy.cpu().numpy()    # (N,4)\n",
    "    track_ids = res.boxes.id.cpu().numpy()      # (N,)\n",
    "    class_ids = res.boxes.cls.cpu().numpy().astype(int)  # (N,)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.imshow(frame_rgb)\n",
    "    ax.axis('off')\n",
    "\n",
    "    team_colors = {0: 'red', 4: 'blue'}  # map your relevant class IDs → colors\n",
    "\n",
    "    # Now zip over the three arrays, using a different name than `cls`\n",
    "    for (x1, y1, x2, y2), tid, cid in zip(boxes, track_ids, class_ids):\n",
    "        # print out for debug\n",
    "        print(f\"Player ID: {tid}, Class: {cid}\")\n",
    "\n",
    "        # only draw if the class is in your team mapping\n",
    "        if cid in team_colors:\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "            color = team_colors[cid]\n",
    "\n",
    "            # draw box\n",
    "            rect = plt.Rectangle(\n",
    "                (x1, y1), w, h,\n",
    "                linewidth=2, edgecolor=color, facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # label with track ID\n",
    "            ax.text(\n",
    "                x1, y1 - 6, f\"Player {int(tid)}\",\n",
    "                color='white', fontsize=7,\n",
    "                bbox=dict(facecolor=color, alpha=0.5, pad=1, linewidth=0)\n",
    "            )\n",
    "\n",
    "    plt.show()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5ae2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output paths\n",
    "video_path = \"./Video/2.mp4\"\n",
    "output_path = \"./Video/2_tracked-2.mp4\"\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define VideoWriter\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# Team color mapping\n",
    "team_colors = {0: (0, 0, 255), 4: (255, 0, 0)}  # Red for class 0, Blue for class 4 (BGR)\n",
    "\n",
    "# Run tracking on the video as a stream\n",
    "results = model.track(source=video_path, persist=True, stream=True, )\n",
    "\n",
    "# Process frame by frame\n",
    "for res in results:\n",
    "    frame = res.orig_img.copy()  # BGR format for saving with OpenCV\n",
    "\n",
    "    if res.boxes.id is None:\n",
    "        out.write(frame)\n",
    "        continue\n",
    "\n",
    "    boxes     = res.boxes.xyxy.cpu().numpy()\n",
    "    track_ids = res.boxes.id.cpu().numpy().astype(int)\n",
    "    class_ids = res.boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "    for (x1, y1, x2, y2), tid, cid in zip(boxes, track_ids, class_ids):\n",
    "        if cid in team_colors:\n",
    "            color = team_colors[cid]\n",
    "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            # Draw label\n",
    "            label = f\"Player {tid}\"\n",
    "            cv2.putText(frame, label, (x1, y1 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    out.write(frame)\n",
    "\n",
    "# Release everything\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"✅ Tracking video saved at:\", output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3311f4f1",
   "metadata": {},
   "source": [
    "## **Tracking the Trajectory of the Player**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63faeb73",
   "metadata": {},
   "source": [
    "Beyond simple tracking, we can visualize player movement by plotting their trajectories. We'll store the center point of each player's bounding box over time and draw lines to create a visual trail showing each player's path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae5805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Input and output video paths\n",
    "video_path = \"./Video/4.mp4\"\n",
    "output_path = \"./Video/4_output_with_trajectory.mp4\"\n",
    "\n",
    "# Open video and get properties\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Output writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# Team color mapping (BGR)\n",
    "team_colors = {0: (0, 0, 255), 4: (255, 0, 0)}  # Red for class 0, Blue for class 4\n",
    "\n",
    "# Dictionary to store trajectory points\n",
    "trajectories = defaultdict(list)\n",
    "\n",
    "# Perform tracking\n",
    "results = model.track(source=video_path, persist=True, stream=True)\n",
    "\n",
    "for res in results:\n",
    "    frame = res.orig_img.copy()\n",
    "\n",
    "    if res.boxes.id is None:\n",
    "        out.write(frame)\n",
    "        continue\n",
    "\n",
    "    boxes     = res.boxes.xyxy.cpu().numpy()\n",
    "    track_ids = res.boxes.id.cpu().numpy().astype(int)\n",
    "    class_ids = res.boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "    for (x1, y1, x2, y2), tid, cid in zip(boxes, track_ids, class_ids):\n",
    "        if cid in team_colors:\n",
    "            color = team_colors[cid]\n",
    "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "            cx, cy = int((x1 + x2) / 2), int((y1 + y2) / 2)\n",
    "\n",
    "            # Update trajectory for this track_id\n",
    "            trajectories[tid].append((cx, cy))\n",
    "\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(frame, f\"Player {tid}\", (x1, y1 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "            # Draw trajectory line (at least 2 points needed)\n",
    "            if len(trajectories[tid]) >= 2:\n",
    "                for j in range(1, len(trajectories[tid])):\n",
    "                    pt1 = trajectories[tid][j - 1]\n",
    "                    pt2 = trajectories[tid][j]\n",
    "                    cv2.line(frame, pt1, pt2, color, 2)\n",
    "\n",
    "    out.write(frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"✅ Video with trajectories saved to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1d6b8b",
   "metadata": {},
   "source": [
    "## **Heatmap of the Players on the ground**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7abf3c",
   "metadata": {},
   "source": [
    "To get a high-level view of player positioning, we can generate a heatmap. This visualization aggregates player locations over the entire video. We create an accumulator for each team and increment the values where players are detected. Finally, we apply a colormap and overlay it on a video frame to see which areas were most occupied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd36eb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "video_path = \"./Video/4.mp4\"\n",
    "\n",
    "classes_of_interest = [0, 4]  # your team classes\n",
    "\n",
    "# Open video to get shape & frame count\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "cap.release()\n",
    "\n",
    "# 2. Initialize accumulators: one 2D float array per class\n",
    "heatmaps = {cls: np.zeros((height, width), dtype=np.float32) \n",
    "            for cls in classes_of_interest}\n",
    "\n",
    "# 3. Run tracking (or detection) over entire video\n",
    "results = model.track(source=video_path, persist=True, stream=True)\n",
    "\n",
    "for res in results:\n",
    "    if res.boxes.id is None:\n",
    "        continue\n",
    "\n",
    "    boxes     = res.boxes.xyxy.cpu().numpy().astype(int)\n",
    "    class_ids = res.boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "    for (x1, y1, x2, y2), cid in zip(boxes, class_ids):\n",
    "        if cid not in classes_of_interest:\n",
    "            continue\n",
    "\n",
    "        # Option A: accumulate box area\n",
    "        heatmaps[cid][y1:y2, x1:x2] += 1\n",
    "\n",
    "        # —OR— Option B: accumulate only the center point\n",
    "        # cx, cy = (x1 + x2)//2, (y1 + y2)//2\n",
    "        # heatmaps[cid][cy, cx] += 1\n",
    "\n",
    "# 4. Normalize & colorize each heatmap\n",
    "colored_maps = {}\n",
    "for cid, hm in heatmaps.items():\n",
    "    # normalize to 0–255\n",
    "    hm_norm = cv2.normalize(hm, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    # apply JET colormap\n",
    "    colored = cv2.applyColorMap(hm_norm, cv2.COLORMAP_JET)\n",
    "    colored_maps[cid] = colored  # BGR image\n",
    "\n",
    "# 5. Overlay on a sample frame (e.g., the first frame)\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 100)\n",
    "\n",
    "ret, base = cap.read()\n",
    "cap.release()\n",
    "if not ret:\n",
    "    raise RuntimeError(\"Failed to read sample frame.\")\n",
    "\n",
    "overlay = base.copy()\n",
    "alpha = 0.5  # transparency\n",
    "\n",
    "for cid, cmap in colored_maps.items():\n",
    "    # blend heatmap with the base frame\n",
    "    cv2.addWeighted(cmap, alpha, overlay, 1 - alpha, 0, overlay)\n",
    "\n",
    "# 6. Save or display\n",
    "cv2.imwrite(\"class_heatmaps_overlay.png\", overlay)\n",
    "print(\"Saved overlay image with heatmaps: per_class_heatmaps_overlay.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1252bb59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune-yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
