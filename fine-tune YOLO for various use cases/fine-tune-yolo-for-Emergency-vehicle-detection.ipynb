{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
    "\n",
    "# **Fine-Tune YOLO for Emergency Vehicle Detection**\n",
    "\n",
    "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog/)\n",
    "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
    "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)\n",
    "\n",
    "## ðŸŽ¯ Objective\n",
    "\n",
    "This notebook provides a step-by-step guide to fine-tuning a YOLO segmentation model to detect emergency vehicles, specifically ambulances. The goal is to create a model that can accurately identify and segment these vehicles in images and videos, which is a crucial component for intelligent traffic management systems.\n",
    "\n",
    "\n",
    "## ðŸš€ Key Features\n",
    "\n",
    "* **Dataset Sourcing**: Guidance on where to find and collect images of emergency vehicles.\n",
    "* **Data Annotation**: Steps for annotating the dataset with segmentation masks using an external tool.\n",
    "* **Format Conversion**: Conversion of standard COCO JSON annotations into the YOLO segmentation format.\n",
    "* **Model Training**: Fine-tuning a pre-trained YOLOv11 segmentation model on the custom ambulance dataset.\n",
    "* **Video Inference**: Applying the trained model to a video to detect and segment emergency vehicles in real-time.\n",
    "\n",
    "\n",
    "## ðŸ“š Libraries & Prerequisites\n",
    "\n",
    "* **Core Libraries**: `ultralytics`.\n",
    "* **Environment**: A Python environment with GPU support (like Google Colab) is highly recommended for efficient model training.\n",
    "* **Dataset**: A custom-collected dataset of emergency vehicle images, an annotation file in COCO format, and test videos for inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create a Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to gather a dataset of images. For this project, we need images of emergency vehicles. The notebook suggests using a specific Kaggle dataset for ambulance images, which serves as our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you have to collect various images of the object you want to detect\n",
    "\n",
    "# In our case, we will collect images of a Ambulance\n",
    "\n",
    "# You can go kaggle to download this dataset of ambulances\n",
    "link = 'https://www.kaggle.com/datasets/aliwannous2021/ambulance-vehicles?select=ambulance2+%28101%29.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Performed Annotation on the Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the images are collected, they need to be annotated. This involves drawing precise segmentation masks (polygons) around the ambulances in each image. The notebook outlines the process of using an external tool like Labellerr for this task and then exporting the annotations in COCO JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to labellerr.com and create a workspace\n",
    "\n",
    "# Upload the images from the output directory\n",
    "\n",
    "# Annotate the images with bounding boxes for PPE items\n",
    "\n",
    "# Export the annotations in COCO JSON format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Convert COCO JSON Annotation to YOLO format**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The YOLO model requires annotations in a specific text format, not COCO JSON. We clone a utility repository that contains a helper script (`seg_converter`) to convert our COCO segmentation annotations into the required YOLO format, making them ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the utility repository to access the required functions\n",
    "!git clone https://github.com/yashsuman15/yolo_finetune_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo_finetune_utils.coco_yolo_converter.seg_converter import coco_to_yolo_converter\n",
    "\n",
    "result = coco_to_yolo_converter(\n",
    "            json_path='./annotation.json',\n",
    "            images_dir='./dataset',\n",
    "            output_dir='yolo_format'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYrzZil_R9e8"
   },
   "source": [
    "### **Install the Ultralytics Package**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the `ultralytics` library, which provides the powerful YOLO framework for training and inference. We install it directly using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-10-08T10:53:42.140929Z",
     "iopub.status.busy": "2024-10-08T10:53:42.140487Z",
     "iopub.status.idle": "2024-10-08T10:53:57.946910Z",
     "shell.execute_reply": "2024-10-08T10:53:57.945769Z",
     "shell.execute_reply.started": "2024-10-08T10:53:42.140885Z"
    },
    "id": "Lrt8kAH2R5IH",
    "outputId": "08082014-c5c9-45f7-aa75-9546b9e168a9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UavftMRSJhe"
   },
   "source": [
    "### **Import All the Requried Libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we import the necessary libraries and run a check to ensure `ultralytics` is set up correctly. We then define the file path to our newly formatted dataset directory, which will be passed to the model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-10-08T10:54:09.803437Z",
     "iopub.status.busy": "2024-10-08T10:54:09.802985Z",
     "iopub.status.idle": "2024-10-08T10:54:31.308005Z",
     "shell.execute_reply": "2024-10-08T10:54:31.306916Z",
     "shell.execute_reply.started": "2024-10-08T10:54:09.803393Z"
    },
    "id": "OKI6XspoSkdc",
    "outputId": "e46b2ba4-48dc-4a6c-abb4-629cf7e88a44",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T10:54:37.319418Z",
     "iopub.status.busy": "2024-10-08T10:54:37.318897Z",
     "iopub.status.idle": "2024-10-08T10:54:37.324139Z",
     "shell.execute_reply": "2024-10-08T10:54:37.323199Z",
     "shell.execute_reply.started": "2024-10-08T10:54:37.319379Z"
    },
    "id": "oHWcOSblSGg8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "execution": {
     "iopub.execute_input": "2024-10-08T10:55:32.254504Z",
     "iopub.status.busy": "2024-10-08T10:55:32.254004Z",
     "iopub.status.idle": "2024-10-08T10:55:32.263896Z",
     "shell.execute_reply": "2024-10-08T10:55:32.262758Z",
     "shell.execute_reply.started": "2024-10-08T10:55:32.254460Z"
    },
    "id": "mdZobxnHVhhf",
    "outputId": "40268334-9043-437c-c6ed-de1cd5374e8e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "location = !pwd\n",
    "dataset_path = f\"{location[0]}/yolo_format\"\n",
    "print(f\"Dataset path: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_cfUufNYF3_"
   },
   "source": [
    "### **Train YOLO11 Model on a Custom Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the core step where we train our custom model. We use the `yolo` command-line interface to start a segmentation training task (`task=segment`). We point it to our dataset's `data.yaml` file, use a pre-trained `yolo11x-seg.pt` model as a starting point, and fine-tune it for 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-10-08T11:01:01.462203Z",
     "iopub.status.busy": "2024-10-08T11:01:01.461737Z",
     "iopub.status.idle": "2024-10-08T11:44:40.134169Z",
     "shell.execute_reply": "2024-10-08T11:44:40.132786Z",
     "shell.execute_reply.started": "2024-10-08T11:01:01.462163Z"
    },
    "id": "yxp_shPlYBAS",
    "outputId": "950df9d7-2ca6-4624-fc38-dfc4264059cb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!yolo task=segment mode=train data={dataset_path}/data.yaml model=\"yolo11x-seg.pt\" epochs=200 imgsz=640 batch=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8XHiRuh7gYJ"
   },
   "source": [
    "### **Inference with Fine-tune on Videos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training is complete, we use our best-trained model weights (`best.pt`) to run inference on a test video. We run a prediction task, set a confidence threshold of 0.7, and specify the source video. The `save=True` argument will save the output video with the detected segmentation masks overlaid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=segment mode=predict model=\"./runs/segment/train/weights/best.pt\" conf=0.7 source=\"./video/g.mp4\" save=True show_labels=False"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "finetune-yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
