{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0511fe5",
   "metadata": {},
   "source": [
    "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
    "\n",
    "# **Fine-Tune YOLO for Threat Detection**\n",
    "\n",
    "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog/)\n",
    "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
    "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)\n",
    "\n",
    "## ðŸŽ¯ Objective\n",
    "\n",
    "This notebook provides a comprehensive workflow for building a threat detection system using a fine-tuned YOLO segmentation model. The project guides you through creating a dataset by extracting frames from videos, training a model to detect weapons like guns and knives, and deploying it to perform real-time inference and highlight potential threats in a video stream.\n",
    "\n",
    "\n",
    "## ðŸš€ Key Features\n",
    "\n",
    "* **Frame Extraction**: Automatically extract image frames from source videos to build a training dataset.\n",
    "* **Format Conversion**: Convert annotations from COCO JSON format to the YOLO segmentation format.\n",
    "* **Model Training**: Fine-tune a YOLOv8 nano segmentation model on a custom threat dataset.\n",
    "* **Video Inference Pipeline**: Develop a complete pipeline to process videos, overlaying bounding boxes, masks, and visual alerts for detected threats.\n",
    "\n",
    "\n",
    "## ðŸ“š Libraries & Prerequisites\n",
    "\n",
    "* **Core Libraries**: `ultralytics`, `opencv-python`.\n",
    "* **Environment**: A Python environment with GPU support is highly recommended for faster model training.\n",
    "* **Dataset**: A video dataset containing various threat objects (e.g., guns, knives) and a corresponding `annotation.json` file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf519a8",
   "metadata": {},
   "source": [
    "### **Create Dataset and Annotation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52143e4",
   "metadata": {},
   "source": [
    "The process begins with creating a dataset. We first clone a utility repository containing a script to extract frames from our source videos at a specified interval. These extracted images are then annotated with segmentation masks for the threat objects, and the annotations are exported in the COCO JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfa4c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect various images of threats like guns from the internet.\n",
    "# Annotate the images using a tool like Labellerr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b626e",
   "metadata": {},
   "source": [
    "### **Convert COCO JSON Annotation to YOLO format**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b63afc",
   "metadata": {},
   "source": [
    "The annotated data in COCO JSON must be converted into the YOLO segmentation format that the model requires for training. We use a helper script from the cloned repository to perform this conversion, which generates the necessary `.txt` label files and the `data.yaml` configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eb4a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Labellerr/yolo_finetune_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a1393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo_finetune_utils.coco_yolo_converter.seg_converter import coco_to_yolo_converter\n",
    "\n",
    "result = coco_to_yolo_converter(\n",
    "            json_path='./annotation.json',\n",
    "            images_dir='./dataset',\n",
    "            output_dir='yolo_format'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640e2d65",
   "metadata": {},
   "source": [
    "### **Train YOLO11 Model on a Custom Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd44c3d",
   "metadata": {},
   "source": [
    "With the dataset correctly formatted, we can now train our YOLO11 nano segmentation model. By starting with the pre-trained `yolo11x-seg.pt` checkpoint, we leverage transfer learning to fine-tune the model on our specific threat detection task for 150 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9466b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4cbbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=train data=\"path/to/dataset.yaml\" model=\"yolo11x.pt\" epochs=200 imgsz=640 batch=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d98e26e",
   "metadata": {},
   "source": [
    "### **Inferencing Fine-Tune YOLO model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4318e1f",
   "metadata": {},
   "source": [
    "After training, we create an inference pipeline to process new videos. We start by loading our best-trained model weights. We then define a function that takes a video frame, runs a prediction, and overlays the resultsâ€”including bounding boxes, class labels, segmentation masks, and a \"Threat Detected\" warningâ€”onto the frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fabbe1d",
   "metadata": {},
   "source": [
    "Finally, we apply this pipeline to an entire video. The code opens the input video, processes each frame using our function, and saves the annotated frames to a new output video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e6e32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the YOLO11 model\n",
    "model = YOLO('./runs/detect/train3/weights/best.pt', task=\"detect\")\n",
    "\n",
    "# Open video capture\n",
    "cap = cv2.VideoCapture(\"./assests/wep2.mp4\")\n",
    "\n",
    "# Define video writer to save output\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "output = cv2.VideoWriter('output.mp4', fourcc, fps, (width, height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run inference on the frame\n",
    "    results = model(frame)\n",
    "    \n",
    "    # Process each detection\n",
    "    for result in results[0].boxes:\n",
    "        # Get box coordinates\n",
    "        x1, y1, x2, y2 = map(int, result.xyxy[0])\n",
    "        \n",
    "        # Get confidence and class\n",
    "        confidence = float(result.conf[0])\n",
    "        class_id = int(result.cls[0])\n",
    "        class_name = \" GUN \"\n",
    "        \n",
    "        if confidence > 0.3:  # Confidence threshold\n",
    "            # Create translucent overlay\n",
    "            overlay = frame.copy()\n",
    "            cv2.rectangle(overlay, (x1, y1), (x2, y2), (0, 0, 255), -1)  # Solid red rectangle\n",
    "            alpha = 0.4  # Transparency factor\n",
    "            cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)\n",
    "\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 1)\n",
    "\n",
    "            # Add label\n",
    "            label = f'{class_name} {confidence *100:.2f}%'\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "\n",
    "    # Write the frame with annotation to output video\n",
    "    output.write(frame)\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "output.release()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune-yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
